{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e974c655-4aa9-4bd3-8d06-3fa71781b479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: https://download.pytorch.org/tutorial/data.zip\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from pdb import set_trace\n",
    "import io\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1ed62c-2724-4227-be11-0f8e3fa77e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabet small + capital letters + \" .,;'\"\n",
    "ALL_LETTERS = string.ascii_letters + \" .,;'\"\n",
    "N_LETTERS = len(ALL_LETTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9c45a4-da1d-498c-b76c-2b17147bb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in ALL_LETTERS\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdf5a94c-afc0-46ba-8165-6ab7af261c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Build the category_lines dictionary, a list of names per language\n",
    "    category_lines = {}\n",
    "    all_categories = []\n",
    "    \n",
    "    def find_files(path):\n",
    "        return glob.glob(path)\n",
    "    \n",
    "    # Read a file and split into lines\n",
    "    def read_lines(filename):\n",
    "        lines = io.open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [unicode_to_ascii(line) for line in lines]\n",
    "    \n",
    "    for filename in find_files('names/*.txt'):\n",
    "        from pdb import set_trace\n",
    "        category = os.path.splitext(os.path.basename(filename))[0]\n",
    "        all_categories.append(category)\n",
    "        \n",
    "        lines = read_lines(filename)\n",
    "        category_lines[category] = lines\n",
    "        \n",
    "    return category_lines, all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ec884b8-26f9-43c0-a612-d1b83038cc14",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-01T13:29:55.853104Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTo represent a single letter, we use a “one-hot vector” of \\nsize <1 x n_letters>. A one-hot vector is filled with 0s\\nexcept for a 1 at index of the current letter, e.g. \"b\" = <0 1 0 0 0 ...>.\\n\\nTo make a word we join a bunch of those into a\\n2D matrix <line_length x 1 x n_letters>.\\n\\nThat extra 1 dimension is because PyTorch assumes\\neverything is in batches - we’re just using a batch size of 1 here.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To represent a single letter, we use a “one-hot vector” of \n",
    "size <1 x n_letters>. A one-hot vector is filled with 0s\n",
    "except for a 1 at index of the current letter, e.g. \"b\" = <0 1 0 0 0 ...>.\n",
    "\n",
    "To make a word we join a bunch of those into a\n",
    "2D matrix <line_length x 1 x n_letters>.\n",
    "\n",
    "That extra 1 dimension is because PyTorch assumes\n",
    "everything is in batches - we’re just using a batch size of 1 here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5778518-e2ef-48f9-b8bd-a02bd61ac333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letter_to_index(letter):\n",
    "    return ALL_LETTERS.find(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e08d2c1c-a613-4cfb-98c3-1a52865c22f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letter_to_tensor(letter):\n",
    "    tensor = torch.zeros(1, N_LETTERS)\n",
    "    tensor[0][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8140a817-5b8d-483a-9bc3-29fc072979ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def line_to_tensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, N_LETTERS)\n",
    "    for i, letter in enumerate(line):\n",
    "        tensor[i][0][letter_to_index(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18487341-8313-419b-9ddd-73bfcb7a5c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_example(category_lines, all_categories):\n",
    "    \n",
    "    def random_choice(a):\n",
    "        random_idx = random.randint(0, len(a) - 1)\n",
    "        return a[random_idx]\n",
    "    \n",
    "    category = random_choice(all_categories)\n",
    "    line = random_choice(category_lines[category])\n",
    "    category_tensor = torch.tensor([all_categories.index(category)], dtype=torch.long)\n",
    "    line_tensor = line_to_tensor(line)\n",
    "    return category, line, category_tensor, line_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd2e603-3f3d-418e-9da4-031ba51852c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        # LSTM expects input in the shape (batch, seq_len, input_size)\n",
    "        # Reshape input_tensor to include batch and seq_len if needed\n",
    "        input_tensor = input_tensor.unsqueeze(1)  # Adding seq_len=1 dimension\n",
    "        output, (hidden, cell) = self.lstm(input_tensor, hidden_state)\n",
    "        \n",
    "        # Pass the LSTM's hidden state through the output layer\n",
    "        output = self.fc(output.squeeze(1))  # Remove the seq_len dimension\n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialize both hidden and cell states as zeros\n",
    "        return (torch.zeros(1, 1, self.hidden_size),  # Hidden state\n",
    "                torch.zeros(1, 1, self.hidden_size))  # Cell state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcc605e6-6b74-46ae-a57d-de29c405aad3",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "category_lines, all_categories = load_data()\n",
    "n_categories = len(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9fc2e1-7e67-43ac-96b7-feb2af307cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_from_output(output):\n",
    "    category_idx = torch.argmax(output).item()\n",
    "    return all_categories[category_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a4ada65-a40a-4d81-af54-7625b1c81859",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "mdl  = LSTM(57, 100, 1)\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6fdfd2b-7d12-4e74-8319-6d139d88c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(line_tensor, category_tensor):\n",
    "    hidden = mdl.init_hidden()\n",
    "    \n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = mdl(line_tensor[i], hidden)\n",
    "        \n",
    "    loss = criterion(output, category_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc648424-84f1-46f4-a274-eb5eb52a45f3",
   "metadata": {},
   "source": [
    "## Make it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4502f6d2-b938-4f1c-aa77-626502443faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_example(category_lines, all_categories):\n",
    "    \n",
    "    def random_choice(a):\n",
    "        random_idx = random.randint(0, len(a) - 1)\n",
    "        return a[random_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b6ed10d-49b3-4590-928b-790e1841681b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_lines_from_files(directory):\n",
    "    results = []  # To store the tuples\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.txt'):  # Process only .txt files\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for line in file:\n",
    "                    line_content = line.strip()  # Remove leading/trailing whitespace\n",
    "                    results.append((line_content, filename.replace('.txt', '')))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf849e9-9c6c-493b-a363-e4e7c1f1ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'nlp/names'\n",
    "lines_with_files = read_lines_from_files(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bff87f06-55a2-445c-acf0-30f22b398998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesDataset:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, language = self.data[idx]\n",
    "        return name, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf3cacc-9426-4897-bb96-413f55ac88f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, lang in lines_with_files:\n",
    "    if lang == 'Russian':\n",
    "        print(lang)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eef2550f-0eb9-499a-943d-66015cd14977",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NamesDataset(lines_with_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ac1776b-5786-4389-85d7-0acb4cbb09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(ds, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f9faa5f-aa7b-466d-8192-795b6d9811db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        # LSTM expects input in the shape (batch, seq_len, input_size)\n",
    "        # Reshape input_tensor to include batch and seq_len\n",
    "        input_tensor = input_tensor.unsqueeze(1)\n",
    "        output, (hidden, cell) = self.lstm(input_tensor, hidden_state)\n",
    "        \n",
    "        # Pass the LSTM's hidden state through the output layer\n",
    "        output = self.fc(output.squeeze(1))  # Remove the seq_len dimension\n",
    "        return output, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialize both hidden and cell states as zeros\n",
    "        return (torch.zeros(1, 1, self.hidden_size),  # Hidden state\n",
    "                torch.zeros(1, 1, self.hidden_size))  # Cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "792f2210-006d-42bf-9ffe-3f6edaab7097",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_letters = len(ALL_LETTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "33985ee3-0eb4-4510-8d2b-44ccebc34eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "mdl = LSTM(num_of_letters, 100, 18)\n",
    "optimizer = torch.optim.Adam(mdl.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "076f1bec-b5f3-454e-ba3d-26c2057b0610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(line_tensor, category_tensor, hidden):\n",
    "    print(line_tensor[0])\n",
    "    raise Exception('a')\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = mdl(line_tensor[i], hidden)\n",
    "    loss = criterion(output, category_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2b3017b7-612a-4521-86f9-25a2f9d9b0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inao\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "a",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:18\u001b[0m\n",
      "Cell \u001b[0;32mIn[74], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(line_tensor, category_tensor, hidden)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(line_tensor, category_tensor, hidden):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line_tensor[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(line_tensor\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      5\u001b[0m         output, hidden \u001b[38;5;241m=\u001b[39m mdl(line_tensor[i], hidden)\n",
      "\u001b[0;31mException\u001b[0m: a"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "j = 0\n",
    "accurate = 0\n",
    "wrong = 0\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "plot_steps = 500\n",
    "num_epochs = 10\n",
    "for i in range(num_epochs):\n",
    "    hidden = mdl.init_hidden()\n",
    "    for name, language in dl:\n",
    "        language = language[0]\n",
    "        name = name[0]\n",
    "        print(name)\n",
    "        category_tensor = torch.tensor([all_categories.index(language)], dtype=torch.long)\n",
    "        line_tensor = line_to_tensor(name)\n",
    "        output, loss = train(line_tensor, category_tensor, hidden)\n",
    "        current_loss += loss \n",
    "\n",
    "        guess = category_from_output(output)\n",
    "        correct = \"CORRECT\" if guess == language else f\"WRONG ({language})\"\n",
    "        \n",
    "        if guess == language:\n",
    "            accurate +=1\n",
    "        else:\n",
    "            wrong +=1    \n",
    "        \n",
    "    if (i+1) % plot_steps == 0:\n",
    "        all_losses.append(current_loss / plot_steps)\n",
    "        current_loss = 0\n",
    "        j+=1\n",
    "        if (j+1) % plot_steps == 100:\n",
    "            print(correct)\n",
    "            print('name, guess and label', name, guess, language)\n",
    "            print('loss', loss)\n",
    "            # print(f\"{i+1} {loss:.4f} {line} / {guess} {correct}\")\n",
    "        # raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2118647b-4409-4484-ac9f-8030b721e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(611, 489)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accurate, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a2101634-3879-45a6-8df9-be81667458d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47363636363636363"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accurate / (accurate + wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "20c8ebef-bd7a-46a7-87ad-c65c058b7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line):\n",
    "    with torch.no_grad():\n",
    "        line_tensor = line_to_tensor(input_line)\n",
    "            \n",
    "        for i in range(line_tensor.size()[0]):\n",
    "            output, _ = mdl(line_tensor[i], hidden)\n",
    "            guess = category_from_output(output)\n",
    "            guess\n",
    "        \n",
    "        return guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1ff6291f-7dc8-46d5-bfe9-b6fc14818118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line):\n",
    "    for i in range(input_line.size()[0]):\n",
    "        output, hidden = mdl(line_tensor[i], hidden)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "6ca86353-d899-4822-853d-cb4e71153f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_names = [\n",
    "    \"Gaber\",\n",
    "    \"Haddad\",\n",
    "    \"Rahal\",\n",
    "    \"Koury\",\n",
    "    \"Harb\",\n",
    "    \"Mikhail\",\n",
    "    \"Dagher\",\n",
    "    \"Shadid\",\n",
    "    \"Boutros\",\n",
    "    \"Mikhail\",\n",
    "    \"Khouri\",\n",
    "    \"Nader\",\n",
    "    \"Issa\",\n",
    "    \"Harb\",\n",
    "    \"Dagher\",\n",
    "    \"Gerges\",\n",
    "    \"Morcos\",\n",
    "    \"Essa\",\n",
    "    \"Fakhoury\",\n",
    "    \"Tuma\",\n",
    "    \"Kattan\",\n",
    "    \"Totah\",\n",
    "    \"Qureshi\",\n",
    "    \"Nahas\",\n",
    "    \"Bitar\",\n",
    "    \"Tahan\",\n",
    "    \"Daher\",\n",
    "    \"Shammas\",\n",
    "    \"Kouri\",\n",
    "    \"Ganim\",\n",
    "    \"Daher\",\n",
    "    \"Awad\",\n",
    "    \"Malouf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c173fb3a-f763-4d30-a8c7-c15658d64c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arabic_names = [\n",
    "#     \"Khoury\", \"Nahas\", \"Daher\", \"Gerges\", \"Nazari\", \"Maalouf\", \"Gerges\", \"Naifeh\", \"Guirguis\", \n",
    "#     \"Baba\", \"Sabbagh\", \"Attia\", \"Tahan\", \"Haddad\", \"Aswad\", \"Najjar\", \"Dagher\", \"Maloof\", \"Isa\", \n",
    "#     \"Asghar\", \"Nader\", \"Gaber\", \"Abboud\", \"Maalouf\", \"Zogby\", \"Srour\", \"Bahar\", \"Mustafa\", \n",
    "#     \"Hanania\", \"Daher\", \"Tuma\", \"Nahas\", \"Saliba\", \"Shamoon\", \"Handal\", \"Baba\", \"Amari\", \"Bahar\", \n",
    "#     \"Atiyeh\", \"Said\", \"Khouri\", \"Tahan\", \"Baba\", \"Mustafa\", \"Guirguis\", \"Sleiman\", \"Seif\", \"Dagher\", \n",
    "#     \"Bahar\", \"Gaber\", \"Harb\", \"Seif\", \"Asker\", \"Nader\", \"Antar\", \"Awad\", \"Srour\", \"Shadid\", \"Hajjar\", \n",
    "#     \"Hanania\", \"Kalb\", \"Shadid\", \"Bazzi\", \"Mustafa\", \"Masih\", \"Ghanem\", \"Haddad\", \"Isa\", \"Antoun\", \n",
    "#     \"Sarraf\", \"Sleiman\", \"Dagher\", \"Najjar\", \"Malouf\", \"Nahas\", \"Naser\", \"Saliba\", \"Shamon\", \"Malouf\", \n",
    "#     \"Kalb\", \"Daher\", \"Maalouf\", \"Wasem\", \"Kanaan\", \"Naifeh\", \"Boutros\", \"Moghadam\", \"Masih\", \"Sleiman\", \n",
    "#     \"Aswad\", \"Cham\", \"Assaf\", \"Quraishi\", \"Shalhoub\", \"Sabbag\", \"Mifsud\", \"Gaber\", \"Shammas\", \"Tannous\", \n",
    "#     \"Sleiman\", \"Bazzi\", \"Quraishi\", \"Rahal\", \"Cham\", \"Ghanem\", \"Ghanem\", \"Naser\", \"Baba\", \"Shamon\", \n",
    "#     \"Almasi\", \"Basara\", \"Quraishi\", \"Bata\", \"Wasem\", \"Shamoun\", \"Deeb\", \"Touma\", \"Asfour\", \"Deeb\", \n",
    "#     \"Hadad\", \"Naifeh\", \"Touma\", \"Bazzi\", \"Shamoun\", \"Nahas\", \"Haddad\", \"Arian\", \"Kouri\", \"Deeb\", \n",
    "#     \"Toma\", \"Halabi\", \"Nazari\", \"Saliba\", \"Fakhoury\", \"Hadad\", \"Baba\", \"Mansour\", \"Sayegh\", \"Antar\", \n",
    "#     \"Deeb\", \"Morcos\", \"Shalhoub\", \"Sarraf\", \"Amari\", \"Wasem\", \"Ganim\", \"Tuma\", \"Fakhoury\", \"Hadad\", \n",
    "#     \"Hakimi\", \"Nader\", \"Said\", \"Ganim\", \"Daher\", \"Ganem\", \"Tuma\", \"Boutros\", \"Aswad\", \"Sarkis\", \"Daher\", \n",
    "#     \"Toma\", \"Boutros\", \"Kanaan\", \"Antar\", \"Gerges\", \"Kouri\", \"Maroun\", \"Wasem\", \"Dagher\", \"Naifeh\", \n",
    "#     \"Bishara\", \"Ba\", \"Cham\", \"Kalb\", \"Bazzi\", \"Bitar\", \"Hadad\", \"Moghadam\", \"Sleiman\", \"Shamoun\", \n",
    "#     \"Antar\", \"Atiyeh\", \"Koury\", \"Nahas\", \"Kouri\", \"Maroun\", \"Nassar\", \"Sayegh\", \"Haik\", \"Ghanem\", \n",
    "#     \"Sayegh\", \"Salib\", \"Cham\", \"Bata\", \"Touma\", \"Antoun\", \"Antar\", \"Bata\", \"Botros\", \"Shammas\", \"Ganim\", \n",
    "#     \"Sleiman\", \"Seif\", \"Moghadam\", \"Ba\", \"Tannous\", \"Bazzi\", \"Seif\", \"Salib\", \"Hadad\", \"Quraishi\", \n",
    "#     \"Halabi\", \"Essa\", \"Bahar\", \"Kattan\", \"Boutros\", \"Nahas\", \"Sabbagh\", \"Kanaan\", \"Sayegh\", \"Said\", \n",
    "#     \"Botros\", \"Najjar\", \"Toma\", \"Bata\", \"Atiyeh\", \"Halabi\", \"Tannous\", \"Kouri\", \"Shamoon\", \"Kassis\", \n",
    "#     \"Haddad\", \"Tuma\", \"Mansour\", \"Antar\", \"Kassis\", \"Kalb\", \"Basara\", \"Rahal\", \"Mansour\", \"Handal\", \n",
    "#     \"Morcos\", \"Fakhoury\", \"Hadad\", \"Morcos\", \"Kouri\", \"Quraishi\", \"Almasi\", \"Awad\", \"Naifeh\", \"Koury\", \n",
    "#     \"Asker\", \"Maroun\", \"Fakhoury\", \"Sabbag\", \"Sarraf\", \"Shamon\", \"Assaf\", \"Boutros\", \"Malouf\", \"Nassar\", \n",
    "#     \"Qureshi\", \"Ghanem\", \"Srour\", \"Almasi\", \"Qureshi\", \"Ghannam\", \"Mustafa\", \"Najjar\", \"Kassab\", \"Shadid\", \n",
    "#     \"Shamoon\", \"Morcos\", \"Atiyeh\", \"Isa\", \"Ba\", \"Baz\", \"Asker\", \"Seif\", \"Asghar\", \"Hajjar\", \"Deeb\", \n",
    "#     \"Essa\", \"Qureshi\", \"Abboud\", \"Ganem\", \"Haddad\", \"Koury\", \"Nassar\", \"Abadi\", \"Toma\", \"Tannous\", \n",
    "#     \"Harb\", \"Issa\", \"Khouri\", \"Mifsud\", \"Kalb\", \"Gaber\", \"Ganim\", \"Boulos\", \"Samaha\", \"Haddad\", \n",
    "#     \"Sabbag\", \"Wasem\", \"Dagher\", \"Rahal\", \"Atiyeh\", \"Antar\", \"Asghar\", \"Mansour\", \"Awad\", \"Boulos\", \n",
    "#     \"Sarraf\", \"Deeb\", \"Abadi\", \"Nazari\", \"Daher\", \"Gerges\", \"Shamoon\", \"Gaber\", \"Amari\", \"Sarraf\", \n",
    "#     \"Nazari\", \"Saliba\", \"Naifeh\", \"Nazari\", \"Hakimi\", \"Shamon\", \"Abboud\", \"Quraishi\", \"Tahan\", \"Safar\", \n",
    "#     \"Hajjar\", \"Srour\", \"Gaber\", \"Shalhoub\", \"Attia\", \"Safar\", \"Said\", \"Ganem\", \"Nader\", \"Asghar\", \n",
    "#     \"Mustafa\", \"Said\", \"Antar\", \"Botros\", \"Nader\", \"Ghannam\", \"Asfour\", \"Tahan\", \"Mansour\", \"Attia\", \n",
    "#     \"Touma\", \"Najjar\", \"Kassis\", \"Abboud\", \"Bishara\", \"Bazzi\", \"Shalhoub\", \"Shalhoub\", \"Safar\", \"Khoury\", \n",
    "#     \"Nazari\", \"Sabbag\", \"Sleiman\", \"Atiyeh\", \"Kouri\", \"Bitar\", \"Zogby\", \"Ghanem\", \"Assaf\", \"Abadi\", \n",
    "#     \"Arian\", \"Shalhoub\", \"Khoury\", \"Morcos\", \"Shamon\", \"Wasem\", \"Abadi\", \"Antoun\", \"Baz\", \"Naser\", \n",
    "#     \"Assaf\", \"Saliba\", \"Nader\", \"Mikhail\", \"Naser\", \"Daher\", \"Morcos\", \"Awad\", \"Nahas\", \"Sarkis\", \n",
    "#     \"Malouf\", \"Mustafa\", \"Fakhoury\", \"Ghannam\", \"Shadid\", \"Gaber\", \"Koury\", \"Atiyeh\", \"Shamon\", \"Boutros\", \n",
    "#     \"Sarraf\", \"Arian\", \"Fakhoury\", \"Abadi\", \"Kassab\", \"Nahas\", \"Quraishi\", \"Mansour\", \"Samaha\", \"Wasem\", \n",
    "#     \"Seif\", \"Fakhoury\", \"Saliba\", \"Cham\", \"Bahar\", \"Shamoun\", \"Essa\", \"Shamon\", \"Asfour\", \"Bitar\", \n",
    "#     \"Cham\", \"Tahan\", \"Tannous\", \"Daher\", \"Khoury\", \"Shamon\", \"Bahar\", \"Quraishi\", \"Ghannam\", \"Kassab\", \n",
    "#     \"Zogby\", \"Basara\", \"Shammas\", \"Arian\", \"Sayegh\", \"Naifeh\", \"Mifsud\", \"Sleiman\", \"Arian\", \"Kassis\", \n",
    "#     \"Shamoun\", \"Kassis\", \"Harb\", \"Mustafa\", \"Boulos\", \"Asghar\", \"Shamon\", \"Kanaan\", \"Atiyeh\", \"Kassab\", \n",
    "#     \"Tahan\", \"Bazzi\", \"Kassis\", \"Qureshi\", \"Basara\", \"Shalhoub\", \"Sayegh\", \"Haik\", \"Attia\", \"Maroun\", \n",
    "#     \"Kassis\", \"Sarkis\", \"Harb\", \"Assaf\", \"Kattan\", \"Antar\", \"Sleiman\", \"Touma\", \"Sarraf\", \"Bazzi\", \n",
    "#     \"Boulos\", \"Baz\", \"Issa\", \"Shamon\", \"Shadid\", \"Deeb\", \"Sabbag\", \"Wasem\", \"Awad\", \"Mansour\", \"Saliba\", \n",
    "#     \"Fakhoury\", \"Arian\", \"Bishara\", \"Dagher\", \"Bishara\", \"Koury\", \"Fakhoury\", \"Naser\", \"Nader\", \"Antar\", \n",
    "#     \"Gerges\", \"Handal\", \"Hanania\", \"Shadid\", \"Gerges\", \"Kassis\", \"Essa\", \"Assaf\", \"Shadid\", \"Seif\", \n",
    "#     \"Shalhoub\", \"Shamoun\", \"Hajjar\", \"Baba\", \"Sayegh\", \"Mustafa\", \"Sabbagh\", \"Isa\", \"Najjar\", \"Tannous\", \n",
    "#     \"Hanania\", \"Ganem\", \"Gerges\", \"Fakhoury\", \"Mifsud\", \"Nahas\", \"Bishara\", \"Bishara\", \"Abadi\", \"Sarkis\", \n",
    "#     \"Masih\", \"Isa\", \"Attia\", \"Kalb\", \"Essa\", \"Boulos\", \"Basara\", \"Halabi\", \"Halabi\", \"Dagher\", \"Attia\", \n",
    "#     \"Kassis\", \"Tuma\", \"Gerges\", \"Ghannam\", \"Toma\", \"Baz\", \"Asghar\", \"Zogby\", \"Aswad\", \"Hadad\", \"Dagher\", \n",
    "#     \"Naser\", \"Shadid\", \"Atiyeh\", \"Zogby\", \"Abboud\", \"Tannous\", \"Khouri\", \"Atiyeh\", \"Ganem\", \"Maalouf\", \n",
    "#     \"Isa\", \"Maroun\", \"Issa\", \"Khouri\", \"Harb\", \"Nader\", \"Awad\", \"Nahas\", \"Said\", \"Baba\", \"Totah\", \"Ganim\", \n",
    "#     \"Handal\", \"Mansour\", \"Basara\", \"Malouf\", \"Said\", \"Botros\", \"Samaha\", \"Safar\", \"Tahan\", \"Botros\", \n",
    "#     \"Shamoun\", \"Handal\", \"Sarraf\", \"Malouf\", \"Bishara\", \"Aswad\", \"Khouri\", \"Baz\", \"Asker\", \"Toma\", \n",
    "#     \"Koury\", \"Gerges\", \"Bishara\", \"Boulos\", \"Najjar\", \"Aswad\", \"Shamon\", \"Kouri\", \"Srour\", \"Assaf\", \n",
    "#     \"Tannous\", \"Attia\", \"Mustafa\", \"Kattan\", \"Asghar\", \"Amari\", \"Shadid\", \"Said\", \"Bazzi\", \"Masih\", \n",
    "#     \"Antar\", \"Fakhoury\", \"Shadid\", \"Masih\", \"Handal\", \"Sarraf\", \"Kassis\", \"Salib\", \"Hajjar\", \"Totah\", \n",
    "#     \"Koury\", \"Totah\", \"Mustafa\", \"Sabbagh\", \"Moghadam\", \"Toma\", \"Srour\", \"Almasi\", \"Totah\", \"Maroun\", \n",
    "#     \"Kattan\", \"Naifeh\", \"Sarkis\", \"Mikhail\", \"Nazari\", \"Boutros\", \"Guirguis\", \"Gaber\", \"Kassis\", \"Masih\", \n",
    "#     \"Hanania\", \"Maloof\", \"Quraishi\", \"Cham\", \"Hadad\", \"Tahan\", \"Bitar\", \"Arian\", \"Gaber\", \"Baz\", \n",
    "#     \"Mansour\", \"Kalb\", \"Sarkis\", \"Attia\", \"Antar\", \"Asfour\", \"Said\", \"Essa\", \"Koury\", \"Hadad\", \"Tuma\", \n",
    "#     \"Moghadam\", \"Sabbagh\", \"Amari\", \"Dagher\", \"Srour\", \"Antoun\", \"Sleiman\", \"Maroun\", \"Tuma\", \"Nahas\", \n",
    "#     \"Hanania\", \"Sayegh\", \"Amari\", \"Sabbagh\", \"Said\", \"Cham\", \"Asker\", \"Nassar\", \"Bitar\", \"Said\", \"Dagher\", \n",
    "#     \"Safar\", \"Khouri\", \"Totah\", \"Khoury\", \"Salib\", \"Basara\", \"Abboud\", \"Baz\", \"Isa\", \"Cham\", \"Amari\", \n",
    "#     \"Mifsud\", \"Hadad\", \"Rahal\", \"Khoury\", \"Bazzi\", \"Basara\", \"Totah\", \"Ghannam\", \"Koury\", \"Malouf\", \n",
    "#     \"Zogby\", \"Zogby\", \"Boutros\", \"Nassar\", \"Handal\", \"Hajjar\", \"Maloof\", \"Abadi\", \"Maroun\", \"Mifsud\", \n",
    "#     \"Kalb\", \"Amari\", \"Hakimi\", \"Boutros\", \"Masih\", \"Kattan\", \"Haddad\", \"Arian\", \"Nazari\", \"Assaf\", \n",
    "#     \"Attia\", \"Wasem\", \"Gerges\", \"Asker\", \"Tahan\", \"Fakhoury\", \"Shadid\", \"Sarraf\", \"Attia\", \"Naifeh\", \n",
    "#     \"Aswad\", \"Deeb\", \"Tannous\", \"Totah\", \"Cham\", \"Baba\", \"Najjar\", \"Hajjar\", \"Shamoon\", \"Handal\", \n",
    "#     \"Awad\", \"Guirguis\", \"Awad\", \"Ganem\", \"Naifeh\", \"Khoury\", \"Hajjar\", \"Moghadam\", \"Mikhail\", \"Ghannam\", \n",
    "#     \"Guirguis\", \"Tannous\", \"Kanaan\", \"Handal\", \"Khoury\", \"Kalb\", \"Qureshi\", \"Najjar\", \"Atiyeh\", \"Gerges\", \n",
    "#     \"Nassar\", \"Tahan\", \"Hadad\", \"Fakhoury\", \"Salib\", \"Wasem\", \"Bitar\", \"Fakhoury\", \"Attia\", \"Awad\", \n",
    "#     \"Totah\", \"Deeb\", \"Touma\", \"Botros\", \"Nazari\", \"Nahas\", \"Kouri\", \"Ghannam\", \"Assaf\", \"Asfour\", \n",
    "#     \"Sarraf\", \"Naifeh\", \"Toma\", \"Asghar\", \"Abboud\", \"Issa\", \"Sabbag\", \"Sabbagh\", \"Isa\", \"Koury\", \n",
    "#     \"Kattan\", \"Shamoon\", \"Rahal\", \"Kalb\", \"Naser\", \"Masih\", \"Sayegh\", \"Dagher\", \"Asker\", \"Maroun\", \n",
    "#     \"Dagher\", \"Sleiman\", \"Botros\", \"Sleiman\", \"Harb\", \"Tahan\", \"Tuma\", \"Said\", \"Hadad\", \"Samaha\", \n",
    "#     \"Harb\", \"Cham\", \"Atiyeh\", \"Haik\", \"Malouf\", \"Bazzi\", \"Harb\", \"Malouf\", \"Ghanem\", \"Cham\", \"Asghar\", \n",
    "#     \"Samaha\", \"Khouri\", \"Nassar\", \"Rahal\", \"Baz\", \"Kalb\", \"Rahal\", \"Gerges\", \"Cham\", \"Sayegh\", \n",
    "#     \"Shadid\", \"Morcos\", \"Shamoon\", \"Hakimi\", \"Shamoon\", \"Qureshi\", \"Ganim\", \"Shadid\", \"Khoury\", \n",
    "#     \"Boutros\", \"Hanania\", \"Antoun\", \"Naifeh\", \"Deeb\", \"Samaha\", \"Awad\", \"Asghar\", \"Awad\", \"Saliba\", \n",
    "#     \"Shamoun\", \"Mikhail\", \"Hakimi\", \"Mikhail\", \"Cham\", \"Halabi\", \"Sarkis\", \"Kattan\", \"Nazari\", \n",
    "#     \"Safar\", \"Morcos\", \"Khoury\", \"Essa\", \"Nassar\", \"Haik\", \"Shadid\", \"Fakhoury\", \"Najjar\", \"Arian\", \n",
    "#     \"Botros\", \"Daher\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "5da4cc09-f546-458e-90ce-9183a086c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# french_names = [\n",
    "#     \"Abel\", \"Abraham\", \"Adam\", \"Albert\", \"Allard\", \"Archambault\", \"Armistead\", \"Arthur\", \"Augustin\", \n",
    "#     \"Babineaux\", \"Baudin\", \"Beauchene\", \"Beaulieu\", \"Beaumont\", \"Bélanger\", \"Bellamy\", \"Bellerose\", \n",
    "#     \"Belrose\", \"Berger\", \"Béringer\", \"Bernard\", \"Bertrand\", \"Bisset\", \"Bissette\", \"Blaise\", \"Blanc\", \n",
    "#     \"Blanchet\", \"Blanchett\", \"Bonfils\", \"Bonheur\", \"Bonhomme\", \"Bonnaire\", \"Bonnay\", \"Bonner\", \n",
    "#     \"Bonnet\", \"Borde\", \"Bordelon\", \"Bouchard\", \"Boucher\", \"Brisbois\", \"Brodeur\", \"Bureau\", \"Caron\", \n",
    "#     \"Cavey\", \"Chaput\", \"Charbonneau\", \"Charpentier\", \"Charron\", \"Chastain\", \"Chevalier\", \"Chevrolet\", \n",
    "#     \"Cloutier\", \"Colbert\", \"Comtois\", \"Cornett\", \"Coté\", \"Coupe\", \"Courtemanche\", \"Cousineau\", \n",
    "#     \"Couture\", \"Daniau\", \"D'aramitz\", \"Daviau\", \"David\", \"Deforest\", \"Degarmo\", \"Delacroix\", \n",
    "#     \"De la fontaine\", \"Deniau\", \"Deniaud\", \"Deniel\", \"Denis\", \"De sauveterre\", \"Deschamps\", \n",
    "#     \"Descoteaux\", \"Desjardins\", \"Desrochers\", \"Desrosiers\", \"Dubois\", \"Duchamps\", \"Dufort\", \"Dufour\", \n",
    "#     \"Duguay\", \"Dupond\", \"Dupont\", \"Durand\", \"Durant\", \"Duval\", \"Émile\", \"Eustis\", \"Fabian\", \"Fabre\", \n",
    "#     \"Fabron\", \"Faucher\", \"Faucheux\", \"Faure\", \"Favager\", \"Favre\", \"Favreau\", \"Fay\", \"Félix\", \"Firmin\", \n",
    "#     \"Fontaine\", \"Forest\", \"Forestier\", \"Fortier\", \"Foss\", \"Fournier\", \"Gage\", \"Gagne\", \"Gagnier\", \n",
    "#     \"Gagnon\", \"Garcon\", \"Gardinier\", \"Germain\", \"Géroux\", \"Giles\", \"Girard\", \"Giroux\", \"Glaisyer\", \n",
    "#     \"Gosse\", \"Gosselin\", \"Granger\", \"Guérin\", \"Guillory\", \"Hardy\", \"Harman\", \"Hébert\", \"Herbert\", \n",
    "#     \"Herriot\", \"Jacques\", \"Janvier\", \"Jordan\", \"Joubert\", \"Labelle\", \"Lachance\", \"Lachapelle\", \n",
    "#     \"Lamar\", \"Lambert\", \"Lane\", \"Langlais\", \"Langlois\", \"Lapointe\", \"Larue\", \"Laurent\", \"Lavigne\", \n",
    "#     \"Lavoie\", \"Leandres\", \"Lebeau\", \"Leblanc\", \"Leclair\", \"Leclerc\", \"Lécuyer\", \"Lefebvre\", \n",
    "#     \"Lefévre\", \"Lefurgey\", \"Legrand\", \"Lemaire\", \"Lémieux\", \"Leon\", \"Leroy\", \"Lesauvage\", \n",
    "#     \"Lestrange\", \"Lévêque\", \"Lévesque\", \"Linville\", \"Lyon\", \"Lyon\", \"Maçon\", \"Marchand\", \"Marie\", \n",
    "#     \"Marion\", \"Martel\", \"Martel\", \"Martin\", \"Masson\", \"Masson\", \"Mathieu\", \"Mercier\", \"Merle\", \n",
    "#     \"Michaud\", \"Michel\", \"Monet\", \"Monette\", \"Montagne\", \"Moreau\", \"Moulin\", \"Mullins\", \"Noel\", \n",
    "#     \"Oliver\", \"Olivier\", \"Page\", \"Paget\", \"Palomer\", \"Pan\", \"Pape\", \"Paquet\", \"Paquet\", \"Parent\", \n",
    "#     \"Paris\", \"Parris\", \"Pascal\", \"Patenaude\", \"Paternoster\", \"Paul\", \"Pelletier\", \"Perrault\", \n",
    "#     \"Perreault\", \"Perrot\", \"Petit\", \"Pettigrew\", \"Pierre\", \"Plamondon\", \"Plourde\", \"Poingdestre\", \n",
    "#     \"Poirier\", \"Porcher\", \"Poulin\", \"Proulx\", \"Renaud\", \"Rey\", \"Reyer\", \"Richard\", \"Richelieu\", \n",
    "#     \"Robert\", \"Roche\", \"Rome\", \"Romilly\", \"Rose\", \"Rousseau\", \"Roux\", \"Roy\", \"Royer\", \"Salomon\", \n",
    "#     \"Salvage\", \"Samson\", \"Samuel\", \"Sargent\", \"Sarkozi\", \"Sarkozy\", \"Sartre\", \"Sault\", \"Sauvage\", \n",
    "#     \"Sauvageau\", \"Sauvageon\", \"Sauvageot\", \"Sauveterre\", \"Savatier\", \"Segal\", \"Sergeant\", \n",
    "#     \"Séverin\", \"Simon\", \"Solomon\", \"Soucy\", \"St martin\", \"St pierre\", \"Tailler\", \"Tasse\", \n",
    "#     \"Thayer\", \"Thibault\", \"Thomas\", \"Tobias\", \"Tolbert\", \"Traver\", \"Travere\", \"Travers\", \"Traverse\", \n",
    "#     \"Travert\", \"Tremblay\", \"Tremble\", \"Victor\", \"Victors\", \"Villeneuve\", \"Vincent\", \"Vipond\", \n",
    "#     \"Voclain\", \"Yount\"\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "12662755-3a84-4e92-93b3-d9a82e7e0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name):\n",
    "    for i in range(name.size()[0]):\n",
    "        output, _ = mdl(name[i], hidden)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "730dcf93-bdbc-4e4d-a5ef-4989eb1d65ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[245], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m wrong \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m french_names:\n\u001b[0;32m----> 4\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrench\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      6\u001b[0m         true \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[244], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(name):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      3\u001b[0m         output, _ \u001b[38;5;241m=\u001b[39m mdl(name[i], hidden)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "true = 0\n",
    "wrong = 0\n",
    "for name in french_names:\n",
    "    res = predict(name)\n",
    "    if res == 'French':\n",
    "        true +=1\n",
    "    else:\n",
    "        wrong +=1\n",
    "print(true, wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "169d9dec-d8c5-4495-bfac-cb4cae3fc32f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[246], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrue\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrue\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mwrong\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print(true/(true+wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b1c13410-7a36-4ef5-938a-01e12b29312a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0383, -0.1426, -0.2375,  0.2879,  1.2915,  0.4320, -1.3325, -0.4204,\n",
       "         -0.3168, -1.2845, -0.1247, -0.3004, -0.1146, -0.4133, -0.3022, -0.1207,\n",
       "          0.9196,  0.0189]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('Wasem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "bcf5cfc1-c08e-40a7-a33a-f8ad3636dea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0383, -0.1426, -0.2375,  0.2879,  1.2915,  0.4320, -1.3325, -0.4204,\n",
       "         -0.3168, -1.2845, -0.1247, -0.3004, -0.1146, -0.4133, -0.3022, -0.1207,\n",
       "          0.9196,  0.0189]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('Samuel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a243d-a5ef-4fbd-8253-c53bc36a4caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     sentence = input(\"Input:\")\n",
    "#     if sentence == \"quit\":\n",
    "#         break\n",
    "    \n",
    "#     predict(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9491eefc-2189-4883-ad75-08db2f4f38df",
   "metadata": {},
   "source": [
    "Antonopoulos\n",
    "Antonopoulos\n",
    "Arvanitoyannis\n",
    "Avgerinos\n",
    "Banos\n",
    "Batsakis\n",
    "Bekyros\n",
    "Belesis\n",
    "Bertsimas\n",
    "Bilias\n",
    "Blades\n",
    "Bouloukos\n",
    "Brisimitzaki"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_purpose_venv",
   "language": "python",
   "name": "all_purpose_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
