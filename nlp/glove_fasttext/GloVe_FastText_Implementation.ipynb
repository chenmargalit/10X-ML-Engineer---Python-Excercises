{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import dok_matrix\n",
    "from pdb import set_trace\n",
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the corpus in a that works. Take this small corpus I built for you, and change it so the meaning is better captured. It will help you understand what the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../datasets/text8', 'r') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.53 s, sys: 629 ms, total: 3.16 s\n",
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%time splitted_txt = raw_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_text = raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_txt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict, Counter\n",
    "# from scipy.sparse import coo_matrix\n",
    "\n",
    "# def build_cooccurrence_matrix_from_string(huge_string, vocab, window_size=5):\n",
    "#     cooccurrences = defaultdict(Counter)\n",
    "    \n",
    "#     # Tokenize the string into words\n",
    "#     tokens = huge_string.split()  # Split by spaces (simple tokenization)\n",
    "#     tokens = [vocab[word] for word in tokens if word in vocab]  # Map to indices\n",
    "    \n",
    "#     # Generate co-occurrences\n",
    "#     for center_idx, center_word in enumerate(tokens):\n",
    "#         start = max(center_idx - window_size, 0)\n",
    "#         end = min(center_idx + window_size + 1, len(tokens))\n",
    "#         for context_idx in range(start, end):\n",
    "#             if center_idx != context_idx:\n",
    "#                 cooccurrences[center_word][tokens[context_idx]] += 1\n",
    "\n",
    "#     # Convert co-occurrences to sparse matrix\n",
    "#     row, col, data = [], [], []\n",
    "#     for word, contexts in cooccurrences.items():\n",
    "#         for context, count in contexts.items():\n",
    "#             row.append(word)\n",
    "#             col.append(context)\n",
    "#             data.append(count)\n",
    "\n",
    "#     return coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def build_cooccurrence_matrix_from_string(huge_string, vocab, window_size=5):\n",
    "    cooccurrences = defaultdict(Counter)\n",
    "    \n",
    "    # Tokenize the string into words\n",
    "    tokens = huge_string.split()  # Split by spaces (simple tokenization)\n",
    "    tokens = [vocab[word] for word in tokens if word in vocab]  # Map to indices\n",
    "    \n",
    "    # Generate co-occurrences\n",
    "    for center_idx, center_word in enumerate(tokens):\n",
    "        start = max(center_idx - window_size, 0)\n",
    "        end = min(center_idx + window_size + 1, len(tokens))\n",
    "        for context_idx in range(start, end):\n",
    "            if center_idx != context_idx:\n",
    "                cooccurrences[center_word][tokens[context_idx]] += 1\n",
    "\n",
    "    # Prepare for sparse matrix construction\n",
    "    row, col, data = [], [], []\n",
    "    vocab_size = len(vocab)\n",
    "    for word, contexts in cooccurrences.items():\n",
    "        if word >= vocab_size:  # Safety check for invalid indices\n",
    "            continue\n",
    "        for context, count in contexts.items():\n",
    "            if context >= vocab_size:  # Safety check for invalid indices\n",
    "                continue\n",
    "            row.append(word)\n",
    "            col.append(context)\n",
    "            data.append(count)\n",
    "\n",
    "    # Convert co-occurrences to sparse matrix\n",
    "    return coo_matrix((data, (row, col)), shape=(vocab_size, vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 µs, sys: 1 µs, total: 14 µs\n",
      "Wall time: 16.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(small_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<21x21 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 0 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_cooccurrence_matrix_from_string(small_text, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe:\n",
    "    def __init__(self, vocab_size, embedding_dim, x_max=100, alpha=0.75):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Initialize word and context embeddings, biases\n",
    "        self.word_embeddings = np.random.rand(vocab_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "        self.context_embeddings = np.random.rand(vocab_size, embedding_dim) / np.sqrt(embedding_dim)\n",
    "        self.word_biases = np.zeros(vocab_size)\n",
    "        self.context_biases = np.zeros(vocab_size)\n",
    "        \n",
    "    def fit(self, cooccurrence_matrix, epochs=50, learning_rate=0.05):\n",
    "        for epoch in range(epochs):\n",
    "            loss = 0\n",
    "            for i, j, Xij in zip(cooccurrence_matrix.row, cooccurrence_matrix.col, cooccurrence_matrix.data):\n",
    "                # Weight function\n",
    "                weight = (Xij / self.x_max)**self.alpha if Xij < self.x_max else 1.0\n",
    "                \n",
    "                # Compute the loss and gradients\n",
    "                word_vec = self.word_embeddings[i]\n",
    "                context_vec = self.context_embeddings[j]\n",
    "                inner_product = np.dot(word_vec, context_vec)\n",
    "                diff = inner_product + self.word_biases[i] + self.context_biases[j] - np.log(Xij)\n",
    "                loss += 0.5 * weight * diff**2\n",
    "                \n",
    "                grad_common = weight * diff\n",
    "                self.word_embeddings[i] -= learning_rate * grad_common * context_vec\n",
    "                self.context_embeddings[j] -= learning_rate * grad_common * word_vec\n",
    "                self.word_biases[i] -= learning_rate * grad_common\n",
    "                self.context_biases[j] -= learning_rate * grad_common\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "            \n",
    "    def get_embeddings(self):\n",
    "        return self.word_embeddings + self.context_embeddings  # Combine embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, vocab, window_size=5):\n",
    "    cooccurrences = defaultdict(Counter)\n",
    "    for sentence in corpus:\n",
    "        tokens= corpus\n",
    "#         tokens = [vocab[word] for word in sentence if word in vocab]\n",
    "        for center_idx, center_word in enumerate(tokens):\n",
    "            start = max(center_idx - window_size, 0)\n",
    "            end = min(center_idx + window_size + 1, len(tokens))\n",
    "            for context_idx in range(start, end):\n",
    "                if center_idx != context_idx:\n",
    "                    cooccurrences[center_word][tokens[context_idx]] += 1\n",
    "\n",
    "    # Convert co-occurrences to sparse matrix\n",
    "    row, col, data = [], [], []\n",
    "    for word, contexts in cooccurrences.items():\n",
    "        for context, count in contexts.items():\n",
    "            row.append(word)\n",
    "            col.append(context)\n",
    "            data.append(count)\n",
    "\n",
    "    return coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(vec1, vec2) / (norm1 * norm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word, vocab, embeddings, top_n=5):\n",
    "    if word not in vocab:\n",
    "        raise ValueError(f\"Word '{word}' not found in vocabulary.\")\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    similarities = [\n",
    "        (other_word, cosine_similarity(word_vec, embeddings[idx]))\n",
    "        for other_word, idx in vocab.items()\n",
    "        if other_word != word\n",
    "    ]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(vocab, embeddings, num_words=50, perplexity=5):\n",
    "\n",
    "\n",
    "    # Limit words to visualize\n",
    "    words = list(vocab.keys())[:num_words]\n",
    "    indices = [vocab[word] for word in words]\n",
    "    word_vectors = embeddings[indices]\n",
    "    \n",
    "    # Adjust perplexity if too few samples\n",
    "    if len(words) <= perplexity:\n",
    "        perplexity = max(len(words) // 2, 1)\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    reduced_vectors = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, word in enumerate(words):\n",
    "        plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])\n",
    "        plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]))\n",
    "    plt.title(\"Word Embedding Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_embedding(sentence, vocab, embeddings):\n",
    "    \"\"\"\n",
    "    Convert a sentence into a single embedding by averaging the word embeddings.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    valid_embeddings = [\n",
    "        embeddings[vocab[word]] for word in words if word in vocab\n",
    "    ]\n",
    "    if not valid_embeddings:\n",
    "        raise ValueError(\"None of the words in the sentence are in the vocabulary.\")\n",
    "    return np.mean(valid_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_a, word_b, word_c, vocab, embeddings, top_n=5):\n",
    "    \"\"\"\n",
    "    Solve word analogy tasks: \"word_a is to word_b as word_c is to ?\".\n",
    "    Example: king - man + woman = queen.\n",
    "    \"\"\"\n",
    "    if word_a not in vocab or word_b not in vocab or word_c not in vocab:\n",
    "        raise ValueError(\"One of the words is not in the vocabulary.\")\n",
    "    \n",
    "    vec_a = embeddings[vocab[word_a]]\n",
    "    vec_b = embeddings[vocab[word_b]]\n",
    "    vec_c = embeddings[vocab[word_c]]\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "    \n",
    "    similarities = [\n",
    "        (word, cosine_similarity(target_vec, embeddings[idx]))\n",
    "        for word, idx in vocab.items()\n",
    "        if word not in {word_a, word_b, word_c}\n",
    "    ]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    ['banana is a fruit'],\n",
    "    ['banana is yellow'],\n",
    "    ['some fruits are yellow'],\n",
    "    ['apple is also a fruit'],\n",
    "    ['banana and apple are fruits'],\n",
    "    ['kiwi is a fruit'],\n",
    "    ['kiwi is also a fruit'],\n",
    "    ['banana and wiki are fruits']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [sentence[0].split() for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['banana', 'is', 'a', 'fruit'],\n",
       " ['banana', 'is', 'yellow'],\n",
       " ['some', 'fruits', 'are', 'yellow'],\n",
       " ['apple', 'is', 'also', 'a', 'fruit'],\n",
       " ['banana', 'and', 'apple', 'are', 'fruits'],\n",
       " ['kiwi', 'is', 'a', 'fruit'],\n",
       " ['kiwi', 'is', 'also', 'a', 'fruit'],\n",
       " ['banana', 'and', 'wiki', 'are', 'fruits']]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_splitted_text = splitted_txt[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 135 µs, sys: 2 µs, total: 137 µs\n",
      "Wall time: 148 µs\n"
     ]
    }
   ],
   "source": [
    "%time vocab = set(small_splitted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 s, sys: 2.15 s, total: 15 s\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = {}\n",
    "for idx, word in enumerate(splitted_txt):\n",
    "    if word not in vocab:\n",
    "        vocab[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "833184"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 s, sys: 2.13 s, total: 19.1 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Build vocabulary from the same corpus used\n",
    "vocab = {word: i for i, word in enumerate(set(word for sentence in splitted_txt for word in sentence))}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.8 s, sys: 62.6 ms, total: 3.87 s\n",
      "Wall time: 3.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = {word: i for i in enumerate(set(splitted_txt))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26 µs, sys: 2 µs, total: 28 µs\n",
      "Wall time: 27.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_to_index = {word: idx for idx, word in enumerate(small_splitted_text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 s, sys: 35.3 s, total: 1min 10s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Filter corpus to ensure only words in vocab are included\n",
    "filtered_corpus = [[word for word in sentence if word in vocab] for sentence in splitted_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_splitted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'anarchism'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn[93], line 21\u001b[0m, in \u001b[0;36mbuild_cooccurrence_matrix\u001b[0;34m(corpus, vocab, window_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m         col\u001b[38;5;241m.\u001b[39mappend(context)\n\u001b[1;32m     19\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(count)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcoo_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/sparse/_coo.py:164\u001b[0m, in \u001b[0;36m_coo_base.__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape \u001b[38;5;241m=\u001b[39m check_shape((M, N))\n\u001b[1;32m    163\u001b[0m idx_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_index_dtype((row, col), maxval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape), check_contents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrow \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(row, copy\u001b[38;5;241m=\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39midx_dtype)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(col, copy\u001b[38;5;241m=\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39midx_dtype)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m getdata(obj, copy\u001b[38;5;241m=\u001b[39mcopy, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'anarchism'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(small_splitted_text, vocab, window_size=5)\n",
    "assert cooccurrence_matrix.shape == (vocab_size, vocab_size), \"Matrix dimensions do not match vocab size.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    ['banana is a fruit'],\n",
    "    ['banana is yellow'],\n",
    "    ['some fruits are yellow'],\n",
    "    ['apple is also a fruit'],\n",
    "    ['banana and apple are fruits'],\n",
    "    ['kiwi is a fruit'],\n",
    "    ['kiwi is also a fruit'],\n",
    "    ['banana and wiki are fruits']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, vocab, window_size=5):\n",
    "    cooccurrences = defaultdict(Counter)\n",
    "    for sentence in corpus:\n",
    "        tokens = [vocab[word] for word in sentence if word in vocab]\n",
    "        for center_idx, center_word in enumerate(tokens):\n",
    "            start = max(center_idx - window_size, 0)\n",
    "            end = min(center_idx + window_size + 1, len(tokens))\n",
    "            for context_idx in range(start, end):\n",
    "                if center_idx != context_idx:\n",
    "                    cooccurrences[center_word][tokens[context_idx]] += 1\n",
    "\n",
    "    # Convert co-occurrences to sparse matrix\n",
    "    row, col, data = [], [], []\n",
    "    for word, contexts in cooccurrences.items():\n",
    "        for context, count in contexts.items():\n",
    "            row.append(word)\n",
    "            col.append(context)\n",
    "            data.append(count)\n",
    "\n",
    "    return coo_matrix((data, (row, col)), shape=(len(vocab), len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25 µs, sys: 1 µs, total: 26 µs\n",
      "Wall time: 31.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 1: Build Vocabulary\n",
    "vocab = {word: i for i, word in enumerate(set(word for sentence in corpus for word in sentence))}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 14.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 2: Filter Corpus to Match Vocabulary\n",
    "filtered_corpus = [[word for word in sentence if word in vocab] for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 237 µs, sys: 216 µs, total: 453 µs\n",
      "Wall time: 459 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Step 3: Build Co-occurrence Matrix\n",
    "cooccurrence_matrix = build_cooccurrence_matrix(filtered_corpus, vocab, window_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.0000\n",
      "Epoch 2/50, Loss: 0.0000\n",
      "Epoch 3/50, Loss: 0.0000\n",
      "Epoch 4/50, Loss: 0.0000\n",
      "Epoch 5/50, Loss: 0.0000\n",
      "Epoch 6/50, Loss: 0.0000\n",
      "Epoch 7/50, Loss: 0.0000\n",
      "Epoch 8/50, Loss: 0.0000\n",
      "Epoch 9/50, Loss: 0.0000\n",
      "Epoch 10/50, Loss: 0.0000\n",
      "Epoch 11/50, Loss: 0.0000\n",
      "Epoch 12/50, Loss: 0.0000\n",
      "Epoch 13/50, Loss: 0.0000\n",
      "Epoch 14/50, Loss: 0.0000\n",
      "Epoch 15/50, Loss: 0.0000\n",
      "Epoch 16/50, Loss: 0.0000\n",
      "Epoch 17/50, Loss: 0.0000\n",
      "Epoch 18/50, Loss: 0.0000\n",
      "Epoch 19/50, Loss: 0.0000\n",
      "Epoch 20/50, Loss: 0.0000\n",
      "Epoch 21/50, Loss: 0.0000\n",
      "Epoch 22/50, Loss: 0.0000\n",
      "Epoch 23/50, Loss: 0.0000\n",
      "Epoch 24/50, Loss: 0.0000\n",
      "Epoch 25/50, Loss: 0.0000\n",
      "Epoch 26/50, Loss: 0.0000\n",
      "Epoch 27/50, Loss: 0.0000\n",
      "Epoch 28/50, Loss: 0.0000\n",
      "Epoch 29/50, Loss: 0.0000\n",
      "Epoch 30/50, Loss: 0.0000\n",
      "Epoch 31/50, Loss: 0.0000\n",
      "Epoch 32/50, Loss: 0.0000\n",
      "Epoch 33/50, Loss: 0.0000\n",
      "Epoch 34/50, Loss: 0.0000\n",
      "Epoch 35/50, Loss: 0.0000\n",
      "Epoch 36/50, Loss: 0.0000\n",
      "Epoch 37/50, Loss: 0.0000\n",
      "Epoch 38/50, Loss: 0.0000\n",
      "Epoch 39/50, Loss: 0.0000\n",
      "Epoch 40/50, Loss: 0.0000\n",
      "Epoch 41/50, Loss: 0.0000\n",
      "Epoch 42/50, Loss: 0.0000\n",
      "Epoch 43/50, Loss: 0.0000\n",
      "Epoch 44/50, Loss: 0.0000\n",
      "Epoch 45/50, Loss: 0.0000\n",
      "Epoch 46/50, Loss: 0.0000\n",
      "Epoch 47/50, Loss: 0.0000\n",
      "Epoch 48/50, Loss: 0.0000\n",
      "Epoch 49/50, Loss: 0.0000\n",
      "Epoch 50/50, Loss: 0.0000\n",
      "CPU times: user 1.29 ms, sys: 924 µs, total: 2.22 ms\n",
      "Wall time: 1.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Step 4: Train GloVe\n",
    "embedding_dim = 50\n",
    "glove = GloVe(vocab_size, embedding_dim)\n",
    "glove.fit(cooccurrence_matrix, epochs=50, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Retrieve Embeddings\n",
    "embeddings = glove.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save embeddings and vocabulary\n",
    "# np.save(\"embeddings.npy\", embeddings)  # Save embeddings\n",
    "# np.save(\"vocab.npy\", vocab)            # Save vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and vocabulary\n",
    "embeddings = np.load(\"embeddings.npy\")\n",
    "vocab = np.load(\"vocab.npy\", allow_pickle=True).item()  # Load as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save entire GloVe model\n",
    "np.savez(\"glove_model.npz\",\n",
    "         word_embeddings=glove.word_embeddings,\n",
    "         context_embeddings=glove.context_embeddings,\n",
    "         word_biases=glove.word_biases,\n",
    "         context_biases=glove.context_biases,\n",
    "         vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entire GloVe model\n",
    "model_data = np.load(\"glove_model.npz\", allow_pickle=True)\n",
    "word_embeddings = model_data[\"word_embeddings\"]\n",
    "context_embeddings = model_data[\"context_embeddings\"]\n",
    "word_biases = model_data[\"word_biases\"]\n",
    "context_biases = model_data[\"context_biases\"]\n",
    "# vocab = model_data[\"vocab\"].item()  # Convert back to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word, vocab, embeddings, top_n=5):\n",
    "    if word not in vocab:\n",
    "        raise ValueError(f\"Word '{word}' not found in vocabulary.\")\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for other_word, idx in vocab.items():\n",
    "        if other_word != word:\n",
    "            other_vec = embeddings[idx]\n",
    "            similarity = cosine_similarity(word_vec, other_vec)\n",
    "            similarities.append((other_word, similarity))\n",
    "    \n",
    "    # Sort by similarity in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word, vocab, embeddings, top_n=5):\n",
    "    if word not in vocab:\n",
    "        raise ValueError(f\"Word '{word}' not found in vocabulary.\")\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for other_word, idx in vocab.items():\n",
    "        if other_word != word:\n",
    "            other_vec = embeddings[idx]\n",
    "            similarity = cosine_similarity(word_vec, other_vec)\n",
    "            similarities.append((other_word, similarity))\n",
    "    \n",
    "    # Sort by similarity in descending order\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['banana is a fruit'],\n",
       " ['banana is yellow'],\n",
       " ['some fruits are yellow'],\n",
       " ['apple is also a fruit'],\n",
       " ['banana and apple are fruits'],\n",
       " ['kiwi is a fruit'],\n",
       " ['kiwi is also a fruit'],\n",
       " ['banana and wiki are fruits']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'some fruits are yellow': 0,\n",
       " 'kiwi is a fruit': 1,\n",
       " 'kiwi is also a fruit': 2,\n",
       " 'apple is also a fruit': 3,\n",
       " 'banana is a fruit': 4,\n",
       " 'banana and apple are fruits': 5,\n",
       " 'banana is yellow': 6,\n",
       " 'banana and wiki are fruits': 7}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'banana':\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Word 'banana' not found in vocabulary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbanana\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords similar to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfind_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m, in \u001b[0;36mfind_similar_words\u001b[0;34m(word, vocab, embeddings, top_n)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_similar_words\u001b[39m(word, vocab, embeddings, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocab:\n\u001b[0;32m----> 3\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in vocabulary.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     word_idx \u001b[38;5;241m=\u001b[39m vocab[word]\n\u001b[1;32m      6\u001b[0m     word_vec \u001b[38;5;241m=\u001b[39m embeddings[word_idx]\n",
      "\u001b[0;31mValueError\u001b[0m: Word 'banana' not found in vocabulary."
     ]
    }
   ],
   "source": [
    "# 1. Find similar words\n",
    "word = \"banana\"\n",
    "print(f\"Words similar to '{word}':\")\n",
    "print(find_similar_words(word, vocab, embeddings, top_n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'tree':\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtree\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords similar to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mfind_similar_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 2. Visualize embeddings\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisualizing embeddings:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[68], line 9\u001b[0m, in \u001b[0;36mfind_similar_words\u001b[0;34m(word, vocab, embeddings, top_n)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in vocabulary.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m word_idx \u001b[38;5;241m=\u001b[39m vocab[word]\n\u001b[0;32m----> 9\u001b[0m word_vec \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m similarities \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     (other_word, cosine_similarity(word_vec, embeddings[idx]))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other_word, idx \u001b[38;5;129;01min\u001b[39;00m vocab\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m other_word \u001b[38;5;241m!=\u001b[39m word\n\u001b[1;32m     14\u001b[0m ]\n\u001b[1;32m     15\u001b[0m similarities\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 2. Visualize embeddings\n",
    "print(\"Visualizing embeddings:\")\n",
    "visualize_embeddings(vocab, embeddings, num_words=6)\n",
    "\n",
    "# # 3. Sentence to embedding\n",
    "# sentence = \"I like learning\"\n",
    "# embedding = sentence_to_embedding(sentence, vocab, final_embeddings)\n",
    "# print(f\"Sentence embedding for '{sentence}': {embedding}\")\n",
    "\n",
    "# # 4. Word analogy\n",
    "# print(\"Word analogy (king - man + woman = ?):\")\n",
    "# print(word_analogy(\"king\", \"man\", \"woman\", vocab, final_embeddings, top_n=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get indices and embeddings of selected words\u001b[39;00m\n\u001b[1;32m      6\u001b[0m indices \u001b[38;5;241m=\u001b[39m [vocab[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m selected_words]\n\u001b[0;32m----> 7\u001b[0m selected_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Visualize using t-SNE\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# Select words to visualize\n",
    "# selected_words = [\"cat\", \"dog\", \"bird\", \"tree\", \"hill\", \"mat\", \"log\", \"snow\", \"fly\"]\n",
    "selected_words = [\"tree\"]\n",
    "\n",
    "# Get indices and embeddings of selected words\n",
    "indices = [vocab[word] for word in selected_words]\n",
    "selected_embeddings = embeddings[indices]\n",
    "\n",
    "# Visualize using t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "reduced_vectors = tsne.fit_transform(selected_embeddings)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, word in enumerate(selected_words):\n",
    "    plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])\n",
    "    plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), fontsize=12)\n",
    "plt.title(\"Word Embedding Visualization (Selected Words)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe and FastText: From Theory to Implementation\n",
    "\n",
    "This notebook provides a comprehensive explanation of GloVe and FastText, two popular word embedding algorithms. We will first cover the theoretical underpinnings of each algorithm, followed by implementing them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GloVe?\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is a word embedding technique that captures global statistical information from a corpus of text. It constructs a word-word co-occurrence matrix and learns word vectors such that their dot products approximate the logarithm of the probabilities of co-occurrence.\n",
    "\n",
    "### Key Ideas:\n",
    "- Uses a co-occurrence matrix to capture the relationship between words.\n",
    "- Optimizes an objective that balances co-occurrence frequency and word similarity.\n",
    "\n",
    "**Objective Function:**\n",
    "$$ J = \\sum_{i,j} f(X_{ij})(w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log(X_{ij}))^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is FastText?\n",
    "\n",
    "FastText extends word embeddings by incorporating subword information (n-grams). This allows it to generate embeddings for rare words and out-of-vocabulary words by aggregating n-gram embeddings.\n",
    "\n",
    "### Key Ideas:\n",
    "- Represents words as a bag of character n-grams.\n",
    "- Embeddings for a word are computed by summing up the embeddings of its n-grams.\n",
    "\n",
    "**Objective Function:**\n",
    "\\[ \\sigma(w_c^T \\cdot w_t) \\] (CBOW) or \\[ \\sigma(w_c^T \\cdot w_{context}) \\] (Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/players_stats_by_season_full_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>League</th>\n",
       "      <th>Season</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>GP</th>\n",
       "      <th>MIN</th>\n",
       "      <th>FGM</th>\n",
       "      <th>FGA</th>\n",
       "      <th>3PM</th>\n",
       "      <th>...</th>\n",
       "      <th>birth_date</th>\n",
       "      <th>height</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>nationality</th>\n",
       "      <th>high_school</th>\n",
       "      <th>draft_round</th>\n",
       "      <th>draft_pick</th>\n",
       "      <th>draft_team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NBA</td>\n",
       "      <td>1999 - 2000</td>\n",
       "      <td>Regular_Season</td>\n",
       "      <td>Shaquille O'Neal</td>\n",
       "      <td>LAL</td>\n",
       "      <td>79</td>\n",
       "      <td>3163.0</td>\n",
       "      <td>956</td>\n",
       "      <td>1665</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Mar 6, 1972</td>\n",
       "      <td>7-1</td>\n",
       "      <td>216.0</td>\n",
       "      <td>325.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Robert G. Cole High School</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Orlando Magic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NBA</td>\n",
       "      <td>1999 - 2000</td>\n",
       "      <td>Regular_Season</td>\n",
       "      <td>Vince Carter</td>\n",
       "      <td>TOR</td>\n",
       "      <td>82</td>\n",
       "      <td>3126.0</td>\n",
       "      <td>788</td>\n",
       "      <td>1696</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>Jan 26, 1977</td>\n",
       "      <td>6-6</td>\n",
       "      <td>198.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Mainland High School</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Golden State Warriors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NBA</td>\n",
       "      <td>1999 - 2000</td>\n",
       "      <td>Regular_Season</td>\n",
       "      <td>Karl Malone</td>\n",
       "      <td>UTA</td>\n",
       "      <td>82</td>\n",
       "      <td>2947.0</td>\n",
       "      <td>752</td>\n",
       "      <td>1476</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>Jul 24, 1963</td>\n",
       "      <td>6-9</td>\n",
       "      <td>206.0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Summerfield High School</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Utah Jazz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NBA</td>\n",
       "      <td>1999 - 2000</td>\n",
       "      <td>Regular_Season</td>\n",
       "      <td>Allen Iverson</td>\n",
       "      <td>PHI</td>\n",
       "      <td>70</td>\n",
       "      <td>2853.0</td>\n",
       "      <td>729</td>\n",
       "      <td>1733</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>Jun 7, 1975</td>\n",
       "      <td>6-0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Bethel High School</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Philadelphia Sixers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NBA</td>\n",
       "      <td>1999 - 2000</td>\n",
       "      <td>Regular_Season</td>\n",
       "      <td>Gary Payton</td>\n",
       "      <td>SEA</td>\n",
       "      <td>82</td>\n",
       "      <td>3425.0</td>\n",
       "      <td>747</td>\n",
       "      <td>1666</td>\n",
       "      <td>177</td>\n",
       "      <td>...</td>\n",
       "      <td>Jul 23, 1968</td>\n",
       "      <td>6-4</td>\n",
       "      <td>193.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>Skyline High School</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Seattle SuperSonics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  League       Season           Stage            Player Team  GP     MIN  FGM  \\\n",
       "0    NBA  1999 - 2000  Regular_Season  Shaquille O'Neal  LAL  79  3163.0  956   \n",
       "1    NBA  1999 - 2000  Regular_Season      Vince Carter  TOR  82  3126.0  788   \n",
       "2    NBA  1999 - 2000  Regular_Season       Karl Malone  UTA  82  2947.0  752   \n",
       "3    NBA  1999 - 2000  Regular_Season     Allen Iverson  PHI  70  2853.0  729   \n",
       "4    NBA  1999 - 2000  Regular_Season       Gary Payton  SEA  82  3425.0  747   \n",
       "\n",
       "    FGA  3PM  ...    birth_date  height  height_cm  weight  weight_kg  \\\n",
       "0  1665    0  ...   Mar 6, 1972     7-1      216.0   325.0      147.0   \n",
       "1  1696   95  ...  Jan 26, 1977     6-6      198.0   220.0      100.0   \n",
       "2  1476    2  ...  Jul 24, 1963     6-9      206.0   265.0      120.0   \n",
       "3  1733   89  ...   Jun 7, 1975     6-0      183.0   165.0       75.0   \n",
       "4  1666  177  ...  Jul 23, 1968     6-4      193.0   180.0       82.0   \n",
       "\n",
       "     nationality                  high_school  draft_round  draft_pick  \\\n",
       "0  United States  Robert G. Cole High School           1.0         1.0   \n",
       "1  United States         Mainland High School          1.0         5.0   \n",
       "2  United States      Summerfield High School          1.0        13.0   \n",
       "3  United States          Bethel High School           1.0         1.0   \n",
       "4  United States          Skyline High School          1.0         2.0   \n",
       "\n",
       "              draft_team  \n",
       "0          Orlando Magic  \n",
       "1  Golden State Warriors  \n",
       "2              Utah Jazz  \n",
       "3    Philadelphia Sixers  \n",
       "4    Seattle SuperSonics  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = df[['Player', 'nationality', 'high_school', 'draft_team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"The player Shaquille O'Neal is from United States, went to the school Robert G. Cole High School      and plays for the team Orlando Magic.\"], ['The player Vince Carter is from United States, went to the school Mainland High School     and plays for the team Golden State Warriors.'], ['The player Karl Malone is from United States, went to the school Summerfield High School     and plays for the team Utah Jazz.']]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus = []\n",
    "for _, row in selected_cols.iterrows():\n",
    "    string = [f\"The player {row['Player']} is from {row['nationality']}, went to the school {row['high_school']} \\\n",
    "    and plays for the team {row['draft_team']}.\"]\n",
    "    corpus.append(string)\n",
    "\n",
    "\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53949"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_corpus = corpus[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of GloVe from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    [\"soccer\", \"is\", \"a\", \"popular\", \"sport\", \"around\", \"the\", \"world\"],\n",
    "    [\"the\", \"goalkeeper\", \"saved\", \"a\", \"penalty\", \"kick\"],\n",
    "    [\"basketball\", \"players\", \"need\", \"to\", \"dribble\", \"the\", \"ball\"],\n",
    "    [\"the\", \"coach\", \"motivates\", \"the\", \"team\", \"to\", \"perform\", \"better\"],\n",
    "    [\"tennis\", \"matches\", \"are\", \"often\", \"played\", \"on\", \"grass\", \"courts\"],\n",
    "    [\"the\", \"referee\", \"blew\", \"the\", \"whistle\", \"to\", \"start\", \"the\", \"match\"],\n",
    "    [\"a\", \"home\", \"run\", \"is\", \"an\", \"exciting\", \"moment\", \"in\", \"baseball\"],\n",
    "    [\"athletes\", \"train\", \"hard\", \"to\", \"compete\", \"in\", \"the\", \"olympics\"],\n",
    "    [\"the\", \"runner\", \"crossed\", \"the\", \"finish\", \"line\", \"to\", \"win\", \"gold\"],\n",
    "    [\"a\", \"football\", \"team\", \"needs\", \"both\", \"offense\", \"and\", \"defense\"],\n",
    "    [\"the\", \"crowd\", \"cheered\", \"loudly\", \"after\", \"the\", \"goal\"],\n",
    "    [\"cricket\", \"is\", \"a\", \"bat-and-ball\", \"game\", \"played\", \"in\", \"many\", \"countries\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soccer', 'is', 'a', 'popular', 'sport', 'around', 'the', 'world']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 281 µs, sys: 2 µs, total: 283 µs\n",
      "Wall time: 291 µs\n"
     ]
    }
   ],
   "source": [
    "%time tokenized_corpus = [sentence.split() for inner_list in small_corpus for sentence in inner_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cold', 'Pershing', 'Armstrong', 'Alonzo', 'Flint']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(word for sentence in tokenized_corpus for word in sentence))\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def build_cooccurrence_matrix(corpus, vocab, window_size=4):\n",
    "    vocab_size = len(vocab)\n",
    "    word_to_id = {word: i for i, word in enumerate(vocab)}\n",
    "    cooccurrence = dok_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "    for sentence in corpus:\n",
    "        for i, word in enumerate(sentence):\n",
    "            for j in range(max(i - window_size, 0), min(i + window_size + 1, len(sentence))):\n",
    "                if i != j:\n",
    "                    cooccurrence[word_to_id[word], word_to_id[sentence[j]]] += 1\n",
    "\n",
    "    return cooccurrence.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['soccer is 90 minutes long', 'soccer is competitive']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_corpus = [\n",
    "    ['soccer is 90 minutes long'],\n",
    "    ['soccer is competitive']\n",
    "]\n",
    "\n",
    "fake_vocab = list(set(word for sentence in fake_corpus for word in sentence))\n",
    "fake_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_cooccurrence_matrix(fake_corpus, fake_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence Matrix:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Co-occurrence Matrix:\")\n",
    "print(cooccurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50  # Dimension of word vectors\n",
    "X_MAX = 100\n",
    "ALPHA = 0.75\n",
    "LEARNING_RATE = 0.05\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embeddings and biases\n",
    "def initialize_glove(vocab_size, embedding_dim):\n",
    "    W = np.random.rand(vocab_size, embedding_dim) * 0.01  # Word vectors\n",
    "    W_context = np.random.rand(vocab_size, embedding_dim) * 0.01  # Context word vectors\n",
    "    b = np.random.rand(vocab_size) * 0.01  # Word biases\n",
    "    b_context = np.random.rand(vocab_size) * 0.01  # Context word biases\n",
    "    return W, W_context, b, b_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 µs, sys: 1e+03 ns, total: 12 µs\n",
      "Wall time: 16.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "# Weighting function\n",
    "def weighting_function(x, x_max=X_MAX, alpha=ALPHA):\n",
    "    return (x / x_max) ** alpha if x < x_max else 1\n",
    "\n",
    "# GloVe training loop\n",
    "def train_glove(cooccurrence, vocab_size, embedding_dim, epochs=EPOCHS, learning_rate=LEARNING_RATE):\n",
    "    W, W_context, b, b_context = initialize_glove(vocab_size, embedding_dim)\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(vocab_size):\n",
    "            for j in range(vocab_size):\n",
    "                X_ij = cooccurrence[i, j]\n",
    "                if X_ij > 0:\n",
    "                    weight = weighting_function(X_ij)\n",
    "                    log_X_ij = np.log(X_ij)\n",
    "                    # Compute the error\n",
    "                    diff = np.dot(W[i], W_context[j]) + b[i] + b_context[j] - log_X_ij\n",
    "                    loss = weight * (diff ** 2)\n",
    "                    total_loss += loss\n",
    "                    \n",
    "                    # Gradients and updates\n",
    "                    grad_wi = weight * diff * W_context[j]\n",
    "                    grad_wj = weight * diff * W[i]\n",
    "                    grad_bi = weight * diff\n",
    "                    grad_bj = weight * diff\n",
    "\n",
    "                    W[i] -= learning_rate * grad_wi\n",
    "                    W_context[j] -= learning_rate * grad_wj\n",
    "                    b[i] -= learning_rate * grad_bi\n",
    "                    b_context[j] -= learning_rate * grad_bj\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}\")\n",
    "    return W + W_context  # Combine word and context embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'coo_matrix' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vocab)\n\u001b[1;32m      3\u001b[0m embedding_dim \u001b[38;5;241m=\u001b[39m EMBEDDING_DIM\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrain_glove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcooccurrence_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<timed exec>:12\u001b[0m, in \u001b[0;36mtrain_glove\u001b[0;34m(cooccurrence, vocab_size, embedding_dim, epochs, learning_rate)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'coo_matrix' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Train GloVe\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = EMBEDDING_DIM\n",
    "train_glove(cooccurrence_matrix, vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_embedding = {word: glove_embeddings[i] for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_word = 'Proviso'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding for a specific word\n",
    "word = test_word  # Example\n",
    "if word in word_to_embedding:\n",
    "    print(f\"Embedding for '{word}': {word_to_embedding[word]}\")\n",
    "else:\n",
    "    print(f\"'{word}' not in vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
    "\n",
    "def find_similar_words(word, top_n=5):\n",
    "    if word not in word_to_embedding:\n",
    "        return f\"'{word}' not in vocabulary.\"\n",
    "\n",
    "    word_vec = word_to_embedding[word]\n",
    "    similarities = {\n",
    "        other_word: cosine_similarity(word_vec, word_to_embedding[other_word])\n",
    "        for other_word in word_to_embedding if other_word != word\n",
    "    }\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_similarities[:top_n]\n",
    "\n",
    "# Example: Find words similar to \"apples\"\n",
    "similar_words = find_similar_words(test_word)\n",
    "print(f\"Words similar to {test_word}:\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings(word_to_embedding, words_to_visualize):\n",
    "    embeddings = np.array([word_to_embedding[word] for word in words_to_visualize])\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, word in enumerate(words_to_visualize):\n",
    "        plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1])\n",
    "        plt.text(reduced_embeddings[i, 0] + 0.01, reduced_embeddings[i, 1] + 0.01, word, fontsize=9)\n",
    "    plt.title(\"Word Embedding Visualization\")\n",
    "    plt.show()\n",
    "\n",
    "# Example: Visualize 10 random words\n",
    "words_to_visualize = list(word_to_embedding.keys())[:10]\n",
    "visualize_embeddings(word_to_embedding, words_to_visualize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_embedding(sentence, word_to_embedding):\n",
    "    embeddings = [word_to_embedding[word] for word in sentence if word in word_to_embedding]\n",
    "    return np.mean(embeddings, axis=0) if embeddings else np.zeros(EMBEDDING_DIM)\n",
    "\n",
    "# Example sentence embedding\n",
    "sentence = [\"i\", \"like\", \"soccer\"]\n",
    "sentence_embedding = sentence_to_embedding(sentence, word_to_embedding)\n",
    "print(\"Sentence Embedding:\", sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word1, word2, word3, word_to_embedding):\n",
    "    if word1 not in word_to_embedding or word2 not in word_to_embedding or word3 not in word_to_embedding:\n",
    "        return \"One of the words is not in the vocabulary.\"\n",
    "\n",
    "    analogy_vec = word_to_embedding[word1] - word_to_embedding[word2] + word_to_embedding[word3]\n",
    "    similarities = {\n",
    "        other_word: cosine_similarity(analogy_vec, word_to_embedding[other_word])\n",
    "        for other_word in word_to_embedding\n",
    "    }\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_similarities[:5]\n",
    "\n",
    "# Example analogy: \"man\" is to \"king\" as \"woman\" is to \"?\"\n",
    "analogy_result = word_analogy(\"soccer\", \"ball\", \"basketball\", word_to_embedding)\n",
    "print(\"Word Analogy Result:\", analogy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(vec1, vec2) / (norm1 * norm2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word, vocab, embeddings, top_n=5):\n",
    "    \"\"\"\n",
    "    Find the most similar words to a given word using cosine similarity.\n",
    "    \"\"\"\n",
    "    if word not in vocab:\n",
    "        raise ValueError(f\"Word '{word}' not found in vocabulary.\")\n",
    "    \n",
    "    word_idx = vocab[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "    similarities = [\n",
    "        (other_word, cosine_similarity(word_vec, embeddings[idx]))\n",
    "        for other_word, idx in vocab.items()\n",
    "        if other_word != word\n",
    "    ]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(vocab, embeddings, num_words=50):\n",
    "    \"\"\"\n",
    "    Visualize embeddings using t-SNE.\n",
    "    \"\"\"\n",
    "    from sklearn.manifold import TSNE\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    words = list(vocab.keys())[:num_words]\n",
    "    indices = [vocab[word] for word in words]\n",
    "    word_vectors = embeddings[indices]\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced_vectors = tsne.fit_transform(word_vectors)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i, word in enumerate(words):\n",
    "        plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])\n",
    "        plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]))\n",
    "    plt.title(\"Word Embedding Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_embedding(sentence, vocab, embeddings):\n",
    "    \"\"\"\n",
    "    Convert a sentence into a single embedding by averaging the word embeddings.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    valid_embeddings = [\n",
    "        embeddings[vocab[word]] for word in words if word in vocab\n",
    "    ]\n",
    "    if not valid_embeddings:\n",
    "        raise ValueError(\"None of the words in the sentence are in the vocabulary.\")\n",
    "    return np.mean(valid_embeddings, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(word_a, word_b, word_c, vocab, embeddings, top_n=5):\n",
    "    \"\"\"\n",
    "    Solve word analogy tasks: \"word_a is to word_b as word_c is to ?\".\n",
    "    Example: king - man + woman = queen.\n",
    "    \"\"\"\n",
    "    if word_a not in vocab or word_b not in vocab or word_c not in vocab:\n",
    "        raise ValueError(\"One of the words is not in the vocabulary.\")\n",
    "    \n",
    "    vec_a = embeddings[vocab[word_a]]\n",
    "    vec_b = embeddings[vocab[word_b]]\n",
    "    vec_c = embeddings[vocab[word_c]]\n",
    "    target_vec = vec_b - vec_a + vec_c\n",
    "    \n",
    "    similarities = [\n",
    "        (word, cosine_similarity(target_vec, embeddings[idx]))\n",
    "        for word, idx in vocab.items()\n",
    "        if word not in {word_a, word_b, word_c}\n",
    "    ]\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of FastText from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The forest is home to many species of animals.\",\n",
    "    \"Rivers provide water for plants and animals.\",\n",
    "    \"Flowers bloom beautifully in the spring.\",\n",
    "    \"The mountain peaks are covered in snow.\",\n",
    "    \"Rain nourishes the earth and helps crops grow.\",\n",
    "    \"The ocean is vast and full of marine life.\",\n",
    "    \"Birds migrate to warmer places in winter.\",\n",
    "    \"Sunsets over the desert are breathtaking.\",\n",
    "    \"The jungle is dense and full of life.\",\n",
    "    \"A calm lake reflects the surrounding trees.\",\n",
    "]\n",
    "\n",
    "# Labels corresponding to the themes in the corpus\n",
    "labels = [\n",
    "    \"forest\",  # Sentence 1\n",
    "    \"river\",   # Sentence 2\n",
    "    \"flower\",  # Sentence 3\n",
    "    \"mountain\",# Sentence 4\n",
    "    \"rain\",    # Sentence 5\n",
    "    \"ocean\",   # Sentence 6\n",
    "    \"bird\",    # Sentence 7\n",
    "    \"desert\",  # Sentence 8\n",
    "    \"jungle\",  # Sentence 9\n",
    "    \"lake\",    # Sentence 10\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 19.0294\n",
      "Epoch 2, Loss: 13.0412\n",
      "Epoch 3, Loss: 8.8429\n",
      "Epoch 4, Loss: 5.5563\n",
      "Epoch 5, Loss: 3.2311\n",
      "Epoch 6, Loss: 1.8025\n",
      "Epoch 7, Loss: 1.0328\n",
      "Epoch 8, Loss: 0.6419\n",
      "Epoch 9, Loss: 0.4383\n",
      "Epoch 10, Loss: 0.3242\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Tokenization and n-gram generation\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def generate_ngrams(word, n=3):\n",
    "    word = f\"<{word}>\"\n",
    "    return [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(corpus, n=3):\n",
    "    vocab = set()\n",
    "    ngram_vocab = set()\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenize(sentence)\n",
    "        vocab.update(tokens)\n",
    "        for token in tokens:\n",
    "            ngram_vocab.update(generate_ngrams(token, n))\n",
    "    return vocab, ngram_vocab\n",
    "\n",
    "vocab, ngram_vocab = build_vocab(corpus)\n",
    "vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "ngram_vocab = {ngram: idx for idx, ngram in enumerate(ngram_vocab)}\n",
    "\n",
    "# Encode sentences and labels\n",
    "def encode_sentence(sentence, vocab, ngram_vocab, n=3):\n",
    "    tokens = tokenize(sentence)\n",
    "    word_indices = [vocab[token] for token in tokens if token in vocab]\n",
    "    ngram_indices = []\n",
    "    for token in tokens:\n",
    "        ngram_indices.extend(\n",
    "            [ngram_vocab[ngram] for ngram in generate_ngrams(token, n) if ngram in ngram_vocab]\n",
    "        )\n",
    "    return word_indices, ngram_indices\n",
    "\n",
    "encoded_data = [\n",
    "    encode_sentence(sentence, vocab, ngram_vocab) for sentence in corpus\n",
    "]\n",
    "label_to_idx = {label: idx for idx, label in enumerate(set(labels))}\n",
    "encoded_labels = [label_to_idx[label] for label in labels]\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "    encoded_data, encoded_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define FastText model\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, ngram_vocab_size, embedding_dim, num_classes):\n",
    "        super(FastText, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.ngram_embeddings = nn.Embedding(ngram_vocab_size, embedding_dim)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, word_indices, ngram_indices):\n",
    "        word_embeds = self.word_embeddings(word_indices).mean(dim=1)\n",
    "        ngram_embeds = self.ngram_embeddings(ngram_indices).mean(dim=1)\n",
    "        combined = word_embeds + ngram_embeds\n",
    "        return self.fc(combined)\n",
    "\n",
    "# Training parameters\n",
    "embedding_dim = 50\n",
    "num_classes = len(label_to_idx)\n",
    "model = FastText(len(vocab), len(ngram_vocab), embedding_dim, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for (word_indices, ngram_indices), label in zip(train_data, train_labels):\n",
    "        word_tensor = torch.tensor(word_indices).unsqueeze(0)\n",
    "        ngram_tensor = torch.tensor(ngram_indices).unsqueeze(0)\n",
    "        label_tensor = torch.tensor([label])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(word_tensor, ngram_tensor)\n",
    "        loss = criterion(output, label_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7])\n",
      "tensor([4])\n",
      "tensor([9])\n",
      "tensor([5])\n",
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for (word_indices, ngram_indices), label in zip(test_data, test_labels):\n",
    "        word_tensor = torch.tensor(word_indices).unsqueeze(0)\n",
    "        ngram_tensor = torch.tensor(ngram_indices).unsqueeze(0)\n",
    "        label_tensor = torch.tensor([label])\n",
    "        \n",
    "        output = model(word_tensor, ngram_tensor)\n",
    "        pred = output.argmax(dim=1)\n",
    "        print(pred)\n",
    "        print(label_tensor)\n",
    "        correct += (pred == label_tensor).sum().item()\n",
    "        total += label_tensor.size(0)\n",
    "\n",
    "print(f\"Accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[24, 48, 5, 15, 10, 43, 11, 37]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "52",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m52\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m37\u001b[39m, \u001b[38;5;241m47\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m13\u001b[39m, \u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m35\u001b[39m]:\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mvocab\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 52"
     ]
    }
   ],
   "source": [
    "for i in [52, 27, 37, 47, 42, 13, 51, 35]:\n",
    "    print(vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {idx: word for word, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [52, 27, 37, 47, 42, 13, 51, 35]:\n",
    "    print(idx_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_label = {idx: label for label, idx in label_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {label: idx for idx, label in enumerate(set(labels))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup and Preprocessing\n",
    "We first tokenize the text, create character n-grams, and build mappings for the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Sample corpus\n",
    "# corpus = [\n",
    "#     \"The forest is home to many species of animals.\",\n",
    "#     \"Rivers provide water for plants and animals.\",\n",
    "#     \"Flowers bloom beautifully in the spring.\",\n",
    "#     \"The mountain peaks are covered in snow.\",\n",
    "#     \"Rain nourishes the earth and helps crops grow.\",\n",
    "#     \"The ocean is vast and full of marine life.\",\n",
    "#     \"Birds migrate to warmer places in winter.\",\n",
    "#     \"Sunsets over the desert are breathtaking.\",\n",
    "#     \"The jungle is dense and full of life.\",\n",
    "#     \"A calm lake reflects the surrounding trees.\",\n",
    "# ]\n",
    "\n",
    "corpus = [\n",
    "    \"Dogs are loyal and friendly animals.\",\n",
    "    \"Cats are independent and curious animals.\",\n",
    "    \"Dogs and cats both make great pets.\",\n",
    "    \"Dogs love to play in the yard.\",\n",
    "    \"Cats love to climb and explore new places.\",\n",
    "    \"Both dogs and cats are part of many families.\",\n",
    "    \"Clouds drift gently across the sky.\",\n",
    "    \"Nature is peaceful and full of life.\",\n",
    "    \"Clouds bring rain, which nourishes nature.\",\n",
    "    \"The beauty of clouds is part of the beauty of nature.\",\n",
    "    \"Clouds form over mountains and forests.\",\n",
    "    \"Nature includes the sky, the earth, and all living things.\",\n",
    "]\n",
    "\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Generate character n-grams\n",
    "def generate_ngrams(word, n=3):\n",
    "    word = f\"<{word}>\"\n",
    "    return [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "\n",
    "# Build vocabulary and n-gram vocabulary\n",
    "def build_vocab(corpus, n=3):\n",
    "    word_counts = Counter()\n",
    "    ngram_vocab = set()\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenize(sentence)\n",
    "        word_counts.update(tokens)\n",
    "        for token in tokens:\n",
    "            ngram_vocab.update(generate_ngrams(token, n))\n",
    "    vocab = {word: idx for idx, word in enumerate(word_counts.keys())}\n",
    "    ngram_vocab = {ngram: idx for idx, ngram in enumerate(ngram_vocab)}\n",
    "    return vocab, ngram_vocab\n",
    "\n",
    "# Build vocabularies\n",
    "vocab, ngram_vocab = build_vocab(corpus)\n",
    "idx_to_word = {idx: word for word, idx in vocab.items()}\n",
    "idx_to_ngram = {idx: ngram for ngram, idx in ngram_vocab.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Skip-Gram Training Data Generation\n",
    "We create training samples by generating target-context pairs for each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate skip-gram pairs\n",
    "def generate_skip_gram_pairs(corpus, vocab, window_size=2):\n",
    "    pairs = []\n",
    "    for sentence in corpus:\n",
    "        tokens = tokenize(sentence)\n",
    "        token_indices = [vocab[token] for token in tokens if token in vocab]\n",
    "        for i, target in enumerate(token_indices):\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(token_indices), i + window_size + 1)\n",
    "            for context in token_indices[start:i] + token_indices[i+1:end]:\n",
    "                pairs.append((target, context))\n",
    "    return pairs\n",
    "\n",
    "skip_gram_pairs = generate_skip_gram_pairs(corpus, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. FastText Model\n",
    "The model combines word and n-gram embeddings for both target and context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, ngram_vocab_size, embedding_dim):\n",
    "        super(FastText, self).__init__()\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.ngram_embeddings = nn.Embedding(ngram_vocab_size, embedding_dim)\n",
    "        self.output_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, target_idx, context_idx, ngram_indices):\n",
    "        # Word embeddings\n",
    "        target_word_embed = self.word_embeddings(target_idx)\n",
    "        context_word_embed = self.output_embeddings(context_idx)\n",
    "\n",
    "        # N-gram embeddings for target word\n",
    "        if len(ngram_indices) > 0:\n",
    "            ngram_embeds = self.ngram_embeddings(torch.tensor(ngram_indices))\n",
    "            target_word_embed += ngram_embeds.mean(dim=0)\n",
    "\n",
    "        # Return dot product\n",
    "        return torch.matmul(target_word_embed, context_word_embed.T).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Negative Sampling Loss\n",
    "We implement the skip-gram loss with negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling_loss(model, target_idx, context_idx, neg_indices, ngram_indices):\n",
    "    # Positive score\n",
    "    pos_score = model(target_idx, context_idx, ngram_indices)\n",
    "    pos_loss = -torch.log(torch.sigmoid(pos_score))\n",
    "\n",
    "    # Negative scores\n",
    "    neg_scores = model(target_idx, neg_indices, ngram_indices)\n",
    "    neg_loss = -torch.sum(torch.log(torch.sigmoid(-neg_scores)))\n",
    "\n",
    "    return pos_loss + neg_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Training Loop\n",
    "The model is trained using skip-gram pairs, negative sampling, and character n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 100\n",
    "num_neg = 5\n",
    "learning_rate = 0.01\n",
    "epochs = 30\n",
    "\n",
    "# Model and optimizer\n",
    "model = FastText(len(vocab), len(ngram_vocab), embedding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 574.7978\n",
      "Epoch 2, Loss: 597.2410\n",
      "Epoch 3, Loss: 662.0351\n",
      "Epoch 4, Loss: 621.6660\n",
      "Epoch 5, Loss: 554.5450\n",
      "Epoch 6, Loss: 526.7131\n",
      "Epoch 7, Loss: 559.4607\n",
      "Epoch 8, Loss: 592.9697\n",
      "Epoch 9, Loss: 560.8083\n",
      "Epoch 10, Loss: 546.5706\n",
      "Epoch 11, Loss: 562.0060\n",
      "Epoch 12, Loss: 517.5833\n",
      "Epoch 13, Loss: 548.4230\n",
      "Epoch 14, Loss: 572.2907\n",
      "Epoch 15, Loss: 585.7272\n",
      "Epoch 16, Loss: 597.3325\n",
      "Epoch 17, Loss: 563.0022\n",
      "Epoch 18, Loss: 571.3578\n",
      "Epoch 19, Loss: 578.0705\n",
      "Epoch 20, Loss: 533.3662\n",
      "Epoch 21, Loss: 561.2059\n",
      "Epoch 22, Loss: 586.1269\n",
      "Epoch 23, Loss: 518.6841\n",
      "Epoch 24, Loss: 493.5637\n",
      "Epoch 25, Loss: 500.7670\n",
      "Epoch 26, Loss: 552.7023\n",
      "Epoch 27, Loss: 576.5710\n",
      "Epoch 28, Loss: 556.9758\n",
      "Epoch 29, Loss: 553.6536\n",
      "Epoch 30, Loss: 551.1399\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    random.shuffle(skip_gram_pairs)\n",
    "    for target, context in skip_gram_pairs:\n",
    "        # Prepare data\n",
    "        target_idx = torch.tensor([target])\n",
    "        context_idx = torch.tensor([context])\n",
    "        neg_indices = torch.randint(0, len(vocab), (num_neg,))\n",
    "        \n",
    "        # Generate n-grams for target word\n",
    "        target_word = idx_to_word[target]\n",
    "        ngram_indices = [ngram_vocab[ngram] for ngram in generate_ngrams(target_word) if ngram in ngram_vocab]\n",
    "\n",
    "        # Compute loss\n",
    "        optimizer.zero_grad()\n",
    "        loss = negative_sampling_loss(model, target_idx, context_idx, neg_indices, ngram_indices)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Word Embeddings\n",
    "After training, you can extract word embeddings directly from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'forest': tensor([ 0.2482,  0.2730, -0.3515, -0.0534,  0.7431, -0.2644, -0.1067, -0.0041,\n",
      "        -0.2399,  0.3199,  0.7748,  0.0358,  0.4299,  0.4340,  0.4636, -0.2827,\n",
      "        -0.4597, -0.0702, -0.5977,  1.7149,  0.7438, -0.6372, -1.3275, -0.3866,\n",
      "         1.4076,  0.8844, -0.5750, -0.5022, -0.4124,  0.9881, -0.6303, -1.5119,\n",
      "         0.5574,  1.3756,  0.1139,  0.5222, -0.7006,  0.6642,  0.7499, -0.1634,\n",
      "        -2.0700, -0.5422, -0.8774,  1.7897,  0.6975,  0.4746,  0.6791,  0.7973,\n",
      "        -0.0936, -0.5318])\n"
     ]
    }
   ],
   "source": [
    "# Retrieve embeddings\n",
    "word_embeddings = model.word_embeddings.weight.data\n",
    "\n",
    "# Example: Get the embedding for \"forest\"\n",
    "word_idx = vocab[\"pets.\"]\n",
    "forest_embedding = word_embeddings[word_idx]\n",
    "print(f\"Embedding for 'forest': {forest_embedding}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Test: Find Similar Words\n",
    "We’ll predict similar words based on their embeddings using cosine similarity.\n",
    "\n",
    "1. Define Cosine Similarity\n",
    "Cosine similarity measures how close two vectors are in a high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Find the Most Similar Words\n",
    "Use the embeddings from the trained model to find the most similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(target_word, model, vocab, top_n=5):\n",
    "    if target_word not in vocab:\n",
    "        return f\"'{target_word}' not in vocabulary.\"\n",
    "\n",
    "    # Get the target word's embedding\n",
    "    target_idx = vocab[target_word]\n",
    "    target_embedding = model.word_embeddings.weight[target_idx]\n",
    "\n",
    "    # Compute similarity with all other words\n",
    "    similarities = {}\n",
    "    for word, idx in vocab.items():\n",
    "        if word == target_word:\n",
    "            continue\n",
    "        word_embedding = model.word_embeddings.weight[idx]\n",
    "        similarity = cosine_similarity(target_embedding, word_embedding)\n",
    "        similarities[word] = similarity\n",
    "\n",
    "    # Sort and return top N most similar words\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_similarities[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run a Prediction Test\n",
    "Use the function to find similar words for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'peaceful':\n",
      "animals.: 0.4202\n",
      "sky.: 0.4082\n",
      "life.: 0.3680\n",
      "of: 0.3386\n",
      "forests.: 0.2859\n"
     ]
    }
   ],
   "source": [
    "# Test for similar words\n",
    "test_word = \"peaceful\"\n",
    "similar_words = find_similar_words(test_word, model, vocab, top_n=5)\n",
    "print(f\"Words similar to '{test_word}':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.word_embeddings.weight.data, \"word_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement Both Algorithms\n",
    "\n",
    "Given the corpus below, implement GloVe and FastText, and evaluate the embeddings generated:\n",
    "\n",
    "### Corpus:\n",
    "```\n",
    "corpus = [\n",
    "    [\"machine\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"deep\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"machine\", \"learning\"]\n",
    "]\n",
    "```\n",
    "### Tasks:\n",
    "- Build the co-occurrence matrix and train GloVe.\n",
    "- Create n-grams and train FastText embeddings.\n",
    "- Compare the embeddings for the word \"learning\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
