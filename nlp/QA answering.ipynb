{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41de858d",
   "metadata": {},
   "source": [
    "# QA Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74623612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541066351d764a04a2d77e08d7e74589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e676d37c319646ceaab412148a00218a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bb6159665e47689ffed86b34b6822a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e589bac6ecc45699c419bc46664c7eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b745e466c4f9457a93f838c696bf61e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who coined the term machine learning?\n",
      "Answer: Arthur Samuel\n",
      "Score: 0.99\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Initialize the QA Pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Step 2: Define Context and Question\n",
    "context = \"\"\"\n",
    "Machine learning is a field of artificial intelligence (AI) that uses statistical techniques to give computer systems \n",
    "the ability to learn from data, without being explicitly programmed. The term was coined in 1959 by Arthur Samuel, \n",
    "an American IBMer and pioneer in the field of computer gaming and artificial intelligence.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who coined the term machine learning?\"\n",
    "\n",
    "# Step 3: Get the Answer\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Step 4: Display the Answer\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Score: {result['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50914a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52814bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Fine-tune the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fictional-qa-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fictional-qa-model\")\n",
    "tokenizer.save_pretrained(\"./fictional-qa-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f5273533",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"version\": \"1.0\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"title\": \"Zorbian History\",\n",
    "      \"paragraphs\": [\n",
    "        {\n",
    "          \"context\": \"Zorbian dynamics was first developed by Dr. Leena Torvak in 2154. Torvak's contributions revolutionized the study of particle energy on Zorbion-5.\",\n",
    "          \"qas\": [\n",
    "            {\n",
    "              \"id\": \"1\",\n",
    "              \"question\": \"Who developed Zorbian dynamics?\",\n",
    "              \"answers\": [\n",
    "                {\n",
    "                  \"text\": \"Dr. Leena Torvak\",\n",
    "                  \"answer_start\": 36\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"context\": \"The Glimstone Treaty of 3098 ended the 400-year conflict between the Xendrians and Ploraxians.\",\n",
    "          \"qas\": [\n",
    "            {\n",
    "              \"id\": \"2\",\n",
    "              \"question\": \"What ended the conflict between the Xendrians and Ploraxians?\",\n",
    "              \"answers\": [\n",
    "                {\n",
    "                  \"text\": \"The Glimstone Treaty\",\n",
    "                  \"answer_start\": 4\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89478500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast, \n",
    "    DistilBertForQuestionAnswering, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from transformers import default_data_collator\n",
    "\n",
    "# Custom QA Dataset\n",
    "qa_data = [\n",
    "    {\n",
    "        'context': \"Machine learning is a field of artificial intelligence (AI) that uses statistical techniques to give computer systems the ability to learn from data, without being explicitly programmed. The term was coined in 1959 by Arthur Samuel, an American IBMer and pioneer in the field of computer gaming and artificial intelligence.\",\n",
    "        'question': \"Who coined the term machine learning?\",\n",
    "        'answer_start': 186,\n",
    "        'answer_text': \"Arthur Samuel\"\n",
    "    },\n",
    "    {\n",
    "        'context': \"Deep learning is a subset of machine learning based on artificial neural networks with representation learning. It can be supervised, semi-supervised or unsupervised. Deep learning architectures such as deep neural networks, deep belief networks, and recurrent neural networks have been applied to fields including computer vision, speech recognition, and natural language processing.\",\n",
    "        'question': \"What is deep learning?\",\n",
    "        'answer_start': 0,\n",
    "        'answer_text': \"Deep learning is a subset of machine learning based on artificial neural networks with representation learning\"\n",
    "    },\n",
    "    {\n",
    "        'context': \"Artificial Intelligence (AI) is intelligence demonstrated by machines, unlike natural intelligence displayed by humans and animals. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\",\n",
    "        'question': \"How is AI research defined?\",\n",
    "        'answer_start': 146,\n",
    "        'answer_text': \"the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e2c5f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is deep learning doing?\n",
      "Answer: by enabling machines to learn from vast amounts of data.\n",
      "Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create DataFrame and then Hugging Face Dataset\n",
    "df = pd.DataFrame(qa_data)\n",
    "features = Features({\n",
    "    'context': Value('string'),\n",
    "    'question': Value('string'),\n",
    "    'answer_start': Value('int32'),\n",
    "    'answer_text': Value('string')\n",
    "})\n",
    "dataset = Dataset.from_pandas(df, features=features)\n",
    "\n",
    "# Load pre-trained Fast tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def prepare_train_features(examples):\n",
    "    # Use tokenizer's built-in QA encoding\n",
    "    tokenized = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Track sample mapping and offset mapping\n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    \n",
    "    tokenized[\"start_positions\"] = []\n",
    "    tokenized[\"end_positions\"] = []\n",
    "    \n",
    "    for i, (sample_idx, offsets) in enumerate(zip(sample_mapping, offset_mapping)):\n",
    "        context = examples['context'][sample_idx]\n",
    "        answer_start = examples['answer_start'][sample_idx]\n",
    "        answer_text = examples['answer_text'][sample_idx]\n",
    "        \n",
    "        # Find token start and end positions for the answer\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        context_index = sequence_ids.index(1)\n",
    "        \n",
    "        # Find start and end character positions\n",
    "        start_char = answer_start\n",
    "        end_char = start_char + len(answer_text)\n",
    "        \n",
    "        # Find token positions corresponding to the answer\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        \n",
    "        token_end_index = len(sequence_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        # Detect answer token positions\n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        \n",
    "        for idx, (start, end) in enumerate(offsets[context_index:], start=context_index):\n",
    "            if start >= start_char and start_token is None:\n",
    "                start_token = idx\n",
    "            if end <= end_char and start <= end_char:\n",
    "                end_token = idx\n",
    "        \n",
    "        # Append token positions\n",
    "        if start_token is not None and end_token is not None:\n",
    "            tokenized[\"start_positions\"].append(start_token)\n",
    "            tokenized[\"end_positions\"].append(end_token)\n",
    "        else:\n",
    "            # Fallback if exact match not found\n",
    "            tokenized[\"start_positions\"].append(context_index)\n",
    "            tokenized[\"end_positions\"].append(context_index)\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Prepare dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_train_features, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_qa_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_qa_model')\n",
    "\n",
    "# Example of using the fine-tuned model\n",
    "from transformers import pipeline\n",
    "\n",
    "fine_tuned_pipeline = pipeline(\n",
    "    \"question-answering\", \n",
    "    model='./fine_tuned_qa_model', \n",
    "    tokenizer='./fine_tuned_qa_model'\n",
    ")\n",
    "\n",
    "# Test the fine-tuned model\n",
    "test_context = \"Deep learning is revolutionizing artificial intelligence by enabling machines to learn from vast amounts of data.\"\n",
    "test_question = \"What is deep learning doing?\"\n",
    "result = fine_tuned_pipeline(question=test_question, context=test_context)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Score: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463b564a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ec187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb393945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce5b0b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who is Zara Elowen?\n",
      "Answer: Elowen's research team recently published groundbreaking results in quantum\n",
      "Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast, \n",
    "    DistilBertForQuestionAnswering, \n",
    "    Trainer, \n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset, Features, Value, Sequence\n",
    "from transformers import default_data_collator\n",
    "from transformers import pipeline\n",
    "\n",
    "# Completely Original Fictional Characters Dataset\n",
    "qa_data = [\n",
    "    {\n",
    "        'context': \"Zara Elowen is a quantum mechanics researcher from the remote mountain city of Aerovia. Born to a family of theoretical physicists, she developed a revolutionary method of quantum entanglement communication that allows instantaneous data transfer across vast distances. Her breakthrough came after years of studying quantum anomalies in her family's hidden laboratory.\",\n",
    "        'question': \"What is Zara Elowen's scientific breakthrough?\",\n",
    "        'answer_start': 138,\n",
    "        'answer_text': \"quantum entanglement communication\"\n",
    "    },\n",
    "    {\n",
    "        'context': \"Kai Rourke is an ex-military strategist turned environmental architect who designs self-sustaining cities in extreme climates. Raised in the arctic regions of New Terra, he developed adaptive building techniques that can withstand temperatures ranging from -50 to 50 degrees Celsius. His modular city designs have been praised for their resilience and minimal environmental impact.\",\n",
    "        'question': \"Where was Kai Rourke raised?\",\n",
    "        'answer_start': 91,\n",
    "        'answer_text': \"arctic regions of New Terra\"\n",
    "    },\n",
    "    {\n",
    "        'context': \"Lyra Voss is a neurotechnology innovator who created a neural interface that allows direct brain-to-computer communication. Her groundbreaking device, called the Synapse Link, enables people with severe motor disabilities to control advanced prosthetics and communicate through thought patterns. She founded her research institute in the island nation of Neuralis after losing her brother in a debilitating accident.\",\n",
    "        'question': \"What is the name of Lyra Voss's neural interface?\",\n",
    "        'answer_start': 125,\n",
    "        'answer_text': \"Synapse Link\"\n",
    "    },\n",
    "    {\n",
    "        'context': \"Ren Kazama is a climate restoration engineer who developed a series of atmospheric manipulation technologies to reverse global warming effects. Originally from the submerged coastal regions of Pacifica, he witnessed firsthand the devastating impacts of rising sea levels. His carbon sequestration algorithms and artificial cloud generation systems have been implemented in multiple global environmental recovery projects.\",\n",
    "        'question': \"What technologies did Ren Kazama develop?\",\n",
    "        'answer_start': 126,\n",
    "        'answer_text': \"atmospheric manipulation technologies\"\n",
    "    },\n",
    "    {\n",
    "        'context': \"Elena Cortez is a quantum computing prodigy who invented a revolutionary algorithm that dramatically reduces computational complexity for machine learning processes. Her work, conducted in the hidden research facilities of the Quantum Collective, allows artificial intelligence systems to learn and adapt at unprecedented speeds. She comes from a long line of mathematical innovators in her family.\",\n",
    "        'question': \"Where did Elena Cortez conduct her research?\",\n",
    "        'answer_start': 146,\n",
    "        'answer_text': \"hidden research facilities of the Quantum Collective\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame and then Hugging Face Dataset\n",
    "df = pd.DataFrame(qa_data)\n",
    "features = Features({\n",
    "    'context': Value('string'),\n",
    "    'question': Value('string'),\n",
    "    'answer_start': Value('int32'),\n",
    "    'answer_text': Value('string')\n",
    "})\n",
    "dataset = Dataset.from_pandas(df, features=features)\n",
    "\n",
    "# Load pre-trained Fast tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function (same as previous script)\n",
    "def prepare_train_features(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "    \n",
    "    tokenized[\"start_positions\"] = []\n",
    "    tokenized[\"end_positions\"] = []\n",
    "    \n",
    "    for i, (sample_idx, offsets) in enumerate(zip(sample_mapping, offset_mapping)):\n",
    "        context = examples['context'][sample_idx]\n",
    "        answer_start = examples['answer_start'][sample_idx]\n",
    "        answer_text = examples['answer_text'][sample_idx]\n",
    "        \n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "        context_index = sequence_ids.index(1)\n",
    "        \n",
    "        start_char = answer_start\n",
    "        end_char = start_char + len(answer_text)\n",
    "        \n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        \n",
    "        token_end_index = len(sequence_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "        \n",
    "        start_token = None\n",
    "        end_token = None\n",
    "        \n",
    "        for idx, (start, end) in enumerate(offsets[context_index:], start=context_index):\n",
    "            if start >= start_char and start_token is None:\n",
    "                start_token = idx\n",
    "            if end <= end_char and start <= end_char:\n",
    "                end_token = idx\n",
    "        \n",
    "        if start_token is not None and end_token is not None:\n",
    "            tokenized[\"start_positions\"].append(start_token)\n",
    "            tokenized[\"end_positions\"].append(end_token)\n",
    "        else:\n",
    "            tokenized[\"start_positions\"].append(context_index)\n",
    "            tokenized[\"end_positions\"].append(context_index)\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Prepare dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    prepare_train_features, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_fictional_characters_model')\n",
    "tokenizer.save_pretrained('./fine_tuned_fictional_characters_model')\n",
    "\n",
    "# Create pipeline with fine-tuned model\n",
    "fine_tuned_pipeline = pipeline(\n",
    "    \"question-answering\", \n",
    "    model='./fine_tuned_fictional_characters_model', \n",
    "    tokenizer='./fine_tuned_fictional_characters_model'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d89c3db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where was Zara Elowen born?\n",
      "Answer: Elowen's research team recently published groundbreaking results in quantum\n",
      "Score: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model with a new query\n",
    "test_context = '''Zara Elowen's research team recently published groundbreaking results in quantum\n",
    "communication technologies. '''\n",
    "# test_context = 'something that is not realted'\n",
    "test_question = \"Where was Zara Elowen born?\"\n",
    "result = fine_tuned_pipeline(question=test_question, context=test_context)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"Score: {result['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77e7dc",
   "metadata": {},
   "source": [
    "This model is useful if for example I give it an entire contract as context and I want to be able to ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d452d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc015e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a2d302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f689f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed516f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e358be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9da4fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_purpose_venv",
   "language": "python",
   "name": "all_purpose_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
