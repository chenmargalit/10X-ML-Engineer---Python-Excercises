{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c82eb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148687e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    return torch.nn.functional.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0145f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995a7428",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'is banana and apple the same?'\n",
    "tokenized_sentence = tokenizer(sentence, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310e964c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2003, 15212,  1998,  6207,  1996,  2168,  1029,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "366c33d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b00eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "406d2e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "025ce6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4db1550",
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_embedding = outputs.last_hidden_state[:, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d141d117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(banana_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a629a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "apple_embedding = outputs.last_hidden_state[:, 4, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f57d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_embedding = outputs.last_hidden_state[:, 6, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28ba5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = cosine_similarity(apple_embedding, banana_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fe0df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embds = outputs.last_hidden_state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf6af95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e685db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_idx = 2\n",
    "apple_idx = 4\n",
    "same_idx = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8d4c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "banana_emd = token_embds[banana_idx]\n",
    "apple_emd = token_embds[apple_idx]\n",
    "same_emd = token_embds[same_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1f0a2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_emd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4adc6fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7752277851104736"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(banana_emd, apple_emd).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7280d435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37262046337127686"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(banana_emd, same_emd).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "81672209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)  # Dot product\n",
    "    norm_vec1 = np.linalg.norm(vec1)  # Magnitude of vec1\n",
    "    norm_vec2 = np.linalg.norm(vec2)  # Magnitude of vec2\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85055cf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7752276"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(banana_emd.detach().numpy(), apple_emd.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deefcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT for sequence classification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Input text\n",
    "text = \"I love this product! It's amazing.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Get predictions\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predicted_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Interpret the result\n",
    "sentiment = \"Positive\" if predicted_class == 1 else \"Negative\"\n",
    "print(f\"Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c6a021e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d332a09861646a3be412b273a5a6893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd02c2ce2b4945c58ea59a6ec1280fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060b4d0b4416410ca5fcd21723a824e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e34c33ecde047e2bc2a030d669453aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "848e4153accb409993661246dfbbbbcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive (Confidence: 1.00)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load tokenizer and pre-trained model fine-tuned for sentiment analysis\n",
    "tokenizer = BertTokenizer.from_pretrained('textattack/bert-base-uncased-SST-2')\n",
    "model = BertForSequenceClassification.from_pretrained('textattack/bert-base-uncased-SST-2')\n",
    "\n",
    "# Input text\n",
    "text = \"I absolutely loved this movie! The story and acting were fantastic.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # Logits for each class (positive, negative)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "predicted_class = torch.argmax(probs).item()\n",
    "\n",
    "# Interpret result\n",
    "classes = [\"Negative\", \"Positive\"]\n",
    "print(f\"Sentiment: {classes[predicted_class]} (Confidence: {probs.max().item():.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe302ff",
   "metadata": {},
   "source": [
    "# Fine Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e9d0a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example data (replace with your dataset)\n",
    "data = {\n",
    "    \"text\": [\n",
    "        \"He scored 30 goals last season and is a top player.\",\n",
    "        \"His performance has been poor with many missed opportunities.\",\n",
    "        \"An incredible midfielder with great passing skills.\",\n",
    "        \"Struggled to make an impact in every game.\",\n",
    "    ],\n",
    "    \"label\": [1, 0, 1, 0],\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"text\"].tolist(), df[\"label\"].tolist(), test_size=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d6d742cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode sequences\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f80a6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SoccerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SoccerDataset(train_encodings, train_labels)\n",
    "val_dataset = SoccerDataset(val_encodings, val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22182693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.331852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.332179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.332846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.37 s, sys: 6.66 s, total: 10 s\n",
      "Wall time: 22.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.0562281608581543, metrics={'train_runtime': 19.6844, 'train_samples_per_second': 0.457, 'train_steps_per_second': 0.152, 'total_flos': 64749986280.0, 'train_loss': 1.0562281608581543, 'epoch': 3.0})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained BERT with a classification head\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # Output directory\n",
    "    num_train_epochs=3,              # Number of epochs\n",
    "    per_device_train_batch_size=8,   # Batch size for training\n",
    "    per_device_eval_batch_size=8,    # Batch size for evaluation\n",
    "    warmup_steps=500,                # Number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # Strength of weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",     # Evaluate at each epoch\n",
    "    save_strategy=\"epoch\",           # Save the model at each epoch\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "038f440f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mcheckpoint-1\u001b[m\u001b[m \u001b[1m\u001b[36mcheckpoint-2\u001b[m\u001b[m \u001b[1m\u001b[36mcheckpoint-3\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "79ab1a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9a363d51b64493bbc0ab18dafc724e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7964ef41e4f54cfd9fd7f3bc8f5f0543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd127f88e2bf4804a25cda7a937135bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "937c194193a34013b0da0dc9ce653306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b688bd2c4cae4b71be620a47d806acec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359af9f402cd468e893cd5945fbb01ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "The COVID-19 pandemic has led to widespread changes in everyday life, including the way people work and interact. Many industries have adopted remote work as the new normal, while others are struggling to adapt.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load BART summarization pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Input text\n",
    "text = \"\"\"\n",
    "The COVID-19 pandemic has led to widespread changes in everyday life, including the way people work and interact. \n",
    "Many industries have adopted remote work as the new normal, while others are struggling to adapt. Vaccination efforts \n",
    "are ramping up globally, but challenges remain in ensuring equitable distribution. The pandemic has also accelerated \n",
    "the adoption of digital technologies in various sectors.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Summary:\")\n",
    "print(summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46fdb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_purpose_venv",
   "language": "python",
   "name": "all_purpose_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
