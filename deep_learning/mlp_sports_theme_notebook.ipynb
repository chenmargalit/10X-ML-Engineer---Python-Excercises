{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons (MLP) - Theory and Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "A Multilayer Perceptron (MLP) is a class of feedforward artificial neural networks. An MLP consists of at least three layers of nodes: an input layer, one or more hidden layers, and an output layer. Each neuron in one layer is connected to every neuron in the next layer, and these connections have associated weights.\n",
    "\n",
    "In this notebook, we will explore the theory and practice of MLPs using examples from the sports domain, such as predicting a player's performance based on their past game statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Use MLP?\n",
    "MLPs are well-suited for problems where the relationship between inputs and outputs is complex and non-linear. By stacking layers and using non-linear activation functions, MLPs can approximate any continuous function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Data\n",
    "We'll simulate some sports data, for example, basketball players' statistics. This will include points, assists, rebounds, etc., and we'll try to predict the player's performance category (excellent, average, or poor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 4), (500,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_samples = 500\n",
    "X = np.random.rand(n_samples, 4) * 100  # 4 features: points, assists, rebounds, steals\n",
    "y = np.random.randint(0, 3, n_samples)  # 3 classes: 0 - poor, 1 - average, 2 - excellent\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54.88135039, 71.51893664, 60.27633761, 54.4883183 ],\n",
       "       [42.36547993, 64.58941131, 43.75872113, 89.17730008],\n",
       "       [96.36627605, 38.34415188, 79.17250381, 52.88949198]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create MLP Model\n",
    "Now, let's create an MLP model using PyTorch. The architecture will include an input layer, two hidden layers, and an output layer corresponding to the three categories of player performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train the Model\n",
    "We'll use CrossEntropyLoss since this is a classification problem. The optimizer will be Stochastic Gradient Descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "hidden_size = 128\n",
    "output_size = 3\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5000\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000], Loss: 1.0633\n",
      "Epoch [200/5000], Loss: 1.0406\n",
      "Epoch [300/5000], Loss: 1.0224\n",
      "Epoch [400/5000], Loss: 1.0049\n",
      "Epoch [500/5000], Loss: 0.9938\n",
      "Epoch [600/5000], Loss: 0.9837\n",
      "Epoch [700/5000], Loss: 0.9728\n",
      "Epoch [800/5000], Loss: 0.9621\n",
      "Epoch [900/5000], Loss: 0.9415\n",
      "Epoch [1000/5000], Loss: 0.9325\n",
      "Epoch [1100/5000], Loss: 0.9265\n",
      "Epoch [1200/5000], Loss: 0.9177\n",
      "Epoch [1300/5000], Loss: 0.9039\n",
      "Epoch [1400/5000], Loss: 0.9062\n",
      "Epoch [1500/5000], Loss: 0.8957\n",
      "Epoch [1600/5000], Loss: 0.8820\n",
      "Epoch [1700/5000], Loss: 0.8785\n",
      "Epoch [1800/5000], Loss: 0.8676\n",
      "Epoch [1900/5000], Loss: 0.8616\n",
      "Epoch [2000/5000], Loss: 0.8640\n",
      "Epoch [2100/5000], Loss: 0.8600\n",
      "Epoch [2200/5000], Loss: 0.8606\n",
      "Epoch [2300/5000], Loss: 0.8618\n",
      "Epoch [2400/5000], Loss: 0.8357\n",
      "Epoch [2500/5000], Loss: 0.8295\n",
      "Epoch [2600/5000], Loss: 0.8224\n",
      "Epoch [2700/5000], Loss: 0.8429\n",
      "Epoch [2800/5000], Loss: 0.8071\n",
      "Epoch [2900/5000], Loss: 0.8333\n",
      "Epoch [3000/5000], Loss: 0.8241\n",
      "Epoch [3100/5000], Loss: 0.7998\n",
      "Epoch [3200/5000], Loss: 0.8008\n",
      "Epoch [3300/5000], Loss: 0.7940\n",
      "Epoch [3400/5000], Loss: 0.7806\n",
      "Epoch [3500/5000], Loss: 0.7588\n",
      "Epoch [3600/5000], Loss: 0.7698\n",
      "Epoch [3700/5000], Loss: 0.7535\n",
      "Epoch [3800/5000], Loss: 0.7527\n",
      "Epoch [3900/5000], Loss: 0.7371\n",
      "Epoch [4000/5000], Loss: 0.7623\n",
      "Epoch [4100/5000], Loss: 0.7624\n",
      "Epoch [4200/5000], Loss: 0.7317\n",
      "Epoch [4300/5000], Loss: 0.7229\n",
      "Epoch [4400/5000], Loss: 0.7111\n",
      "Epoch [4500/5000], Loss: 0.7491\n",
      "Epoch [4600/5000], Loss: 0.7097\n",
      "Epoch [4700/5000], Loss: 0.7116\n",
      "Epoch [4800/5000], Loss: 0.7487\n",
      "Epoch [4900/5000], Loss: 0.7066\n",
      "Epoch [5000/5000], Loss: 0.6856\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.tensor(X, dtype=torch.float32)\n",
    "y_train = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we have learned how to build a basic MLP model to predict player performance based on their statistics. By using MLP's powerful representation learning ability, we can model non-linear relationships, making it a powerful tool for sports analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your own MLP. Take this input data, and this output data, and try to build an MLP that trains. How do you know that it trains? The loss decreases.\n",
    "\n",
    "Try to build a 5 layers MLP, with at least one ReLU. You can use the same loss function and optimizer as we've seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 6), (1000,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "n_samples = 1000\n",
    "X = np.random.rand(n_samples, 6) * 100  # 6 features\n",
    "y = np.random.randint(0, 4, n_samples)  # 4 classes\n",
    "\n",
    "X.shape, y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
