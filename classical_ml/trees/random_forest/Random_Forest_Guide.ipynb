{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411fcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b9157",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Random Forest: What is it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9830fe",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random forest is an ensemble model that is used both for classification and regression. Its built on top of decision trees, often reducing overfit and increasing accuracy. It uses a process called bagging do to so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5514455",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Important Random Forest Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e39840",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 1. Ensemble Learning:\n",
    "- Random Forests uses ensembles. Ensemble is when we use multiplie models to make the same decision, not just one. \n",
    "- In the case on Random Forest, instead of using one decision tree, we use many and average teh result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b321194",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 2. Bagging\n",
    "- Each training is done on a different subset of the dataset, created randomlly by sampling with replacement. This method is known as bootstrap sample. By training in such a way, Random Forest is less overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b87630",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 3. Random Subset of Features:\n",
    "- Random Forest randomly selects a subset of features at each split point. This also helps with avoiding overfit as it prevents from a few dominate features to control the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774b95a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 4. Voting (Classification) / Averaging (Regression):\n",
    "- When performing a classfication task, each tree votes for some class. The final decision is based on majority votes across all trees.\n",
    "- When performing regression, each tree predicts a numeric value and the final decision is an average of all predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392b9e1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 5. Feature Importance:\n",
    "- Random Forest provides a measure of feature importance by calculating how much each feature reduces impurity.\n",
    "- Features that usually reduce impurity across many trees, are reported as more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a95a1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "## Hyperparameters in Random Forest:\n",
    "\n",
    "There are several important hyperparameters to tune when working with a Random Forest model:\n",
    "\n",
    "1. **n_estimators**:  \n",
    "   - The number of decision trees in the forest. More trees generally improve accuracy but increase computation time.\n",
    "\n",
    "2. **max_depth**: \n",
    "   - The maximum depth of each tree. A deeper tree might capture more patterns but risks overfitting.\n",
    "\n",
    "3. **min_samples_split**: \n",
    "   - The minimum number of samples required to split a node. Increasing this value can reduce overfitting by making trees less complex.\n",
    "\n",
    "4. **min_samples_leaf**: \n",
    "   - The minimum number of samples required to be at a leaf node. A larger number helps to smooth predictions.\n",
    "\n",
    "5. **max_features**: \n",
    "   - The maximum number of features to consider when looking for the best split. This can be set as a proportion of the total number of features or a fixed number.\n",
    "\n",
    "6. **bootstrap**: \n",
    "   - Whether to use bootstrap samples when building trees (default is `True`).\n",
    "\n",
    "7. **criterion**: \n",
    "   - The function to measure the quality of a split. It can be \"gini\" (default) for classification or \"entropy\" for classification, and \"mse\" (mean squared error) or \"mae\" (mean absolute error) for regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac82e1f0",
   "metadata": {
    "hidden": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9baf2925",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Sklearn Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628ce37e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8997a222",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petal width (cm): 0.4340\n",
      "petal length (cm): 0.4173\n",
      "sepal length (cm): 0.1041\n",
      "sepal width (cm): 0.0446\n"
     ]
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "\n",
    "sorted_features = sorted(zip(data.feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for feature, importance in sorted_features:\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c2c10",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "## Random Forest Use Cases:\n",
    "\n",
    "1. **Classification**: Medical diagnosis, fraud detection, document classification.\n",
    "2. **Regression**: Predicting house prices, stock prices, or any continuous output.\n",
    "3. **Feature Selection**: As Random Forest provides feature importance, it can be used to select the most important features before applying other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec466f7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Random Forest is a solid option for both classification and regression tasks. It offers a nice balance between complexity and performance, its reasonably powerful in many scenarios. However, it is not always the best fit. Its complexity can make it hard to interpret, so if you need clear explanations or transparency, it might not be the ideal choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df55e57",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Random Forest From Half Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e82e955",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class RandomForestRegressorScratch:\n",
    "    def __init__(self, n_trees=100, max_features=None, max_depth=None, min_samples_split=2):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_features = max_features\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "    \n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "    \n",
    "    def _fit_tree(self, X, y):\n",
    "        tree = DecisionTreeRegressor(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "        tree.fit(X, y)\n",
    "        return tree\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_trees):\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree = self._fit_tree(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d51b28",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the Random Forest from scratch: 0.304552296345607\n"
     ]
    }
   ],
   "source": [
    "housing_data = fetch_california_housing()\n",
    "X, y = housing_data.data, housing_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_scratch = RandomForestRegressorScratch(n_trees=10, max_depth=10)\n",
    "rf_scratch.fit(X_train, y_train)\n",
    "y_pred_rf_scratch = rf_scratch.predict(X_test)\n",
    "\n",
    "mse_rf_scratch = mean_squared_error(y_test, y_pred_rf_scratch)\n",
    "print(f'Mean Squared Error of the Random Forest from scratch: {mse_rf_scratch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26497c52",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Random Forest vs Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da60fb82",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.526138102652923, 0.2571833084850122)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data = fetch_california_housing()\n",
    "X, y = housing_data.data, housing_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "decision_tree = DecisionTreeRegressor(random_state=42)\n",
    "random_forest = RandomForestRegressor(random_state=42)\n",
    "\n",
    "decision_tree.fit(X_train, y_train)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = decision_tree.predict(X_test)\n",
    "y_pred_forest = random_forest.predict(X_test)\n",
    "\n",
    "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
    "mse_forest = mean_squared_error(y_test, y_pred_forest)\n",
    "\n",
    "mse_tree, mse_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36997e5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Random Forest performs better than a single Decision Tree for several key reasons:**\n",
    "\n",
    "1. Ensemble Averaging: Random Forest is an ensemble of multiple Decision Trees. Each tree is trained on a different subset of the data (using bootstrapping) and a random subset of features, which leads to a more diverse set of trees. The final prediction is the average (for regression) or majority vote (for classification) across all trees, reducing the variance in the predictions.\n",
    "\n",
    "2. Overfitting Prevention: Decision Trees tend to overfit on noisy or complex datasets, capturing even minor details in the training data that don't generalize well to unseen data. Random Forest mitigates this by averaging the predictions from many different trees, which smooths out the noise and prevents overfitting.\n",
    "\n",
    "3. Feature Randomization: In Random Forest, at each split, only a random subset of features is considered, preventing any single feature from dominating the model and leading to better generalization. Decision Trees, on the other hand, may focus too heavily on a few specific features, leading to overfitting.\n",
    "\n",
    "4. Reduction of Model Variance: Single Decision Trees can have high variance—meaning that small changes in the training data can result in a significantly different tree. Random Forest reduces this variance by averaging the results of many trees, leading to more stable and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be59cb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Why Do We Like Random Forest\n",
    "\n",
    "- Less Likely to Overfit: Random Forests average multiple decision trees, which helps prevent the common issue of overfitting that often happens with single decision trees.\n",
    "\n",
    "- Deals with Missing Data Well: They can handle missing values in both training and test data without losing much performance.\n",
    "\n",
    "- Handles Big Data: They work well with large datasets, even when there are many features.\n",
    "\n",
    "- Works Well with Imbalanced Classes: They tend to perform better than some other models on datasets with class imbalance by averaging predictions from all trees.\n",
    "\n",
    "- Feature Importance Insight: Random Forests naturally rank the importance of features, helping you understand which variables matter most.\n",
    "\n",
    "## Why Dont We Like Random Forest\n",
    "\n",
    "\n",
    "- Harder to Interpret: Single decision trees are easy to understand, but with a Random Forest combining lots of trees, it becomes more complex to interpret the overall model.\n",
    "\n",
    "- Training Can Be Slow: Building multiple trees takes more time compared to training just one, especially as the number of trees increases.\n",
    "\n",
    "- Slower Predictions: Making predictions can take longer because every tree in the forest needs to be evaluated.\n",
    "\n",
    "- Higher Memory Usage: Random Forests need more memory since they store many trees compared to just one in simpler models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20384252",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
