{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "531a15fd",
   "metadata": {},
   "source": [
    "# Random Forest - an Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df291c5e",
   "metadata": {},
   "source": [
    "Random Forest is a powerful ensemble learning method used in both classification and regression tasks. It's an extension of decision trees, improving their performance by reducing overfitting and increasing accuracy through a process called \"bagging\" (Bootstrap Aggregating)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a46051",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a002233d",
   "metadata": {},
   "source": [
    "Random Forest is based on the concept of ensemble learning, where multiple models (in this case, decision trees) are trained and their outputs are combined to make a more accurate and robust prediction.\n",
    "Instead of relying on one decision tree, which might overfit or generalize poorly, Random Forest averages the results of many trees to improve predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d617b7",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b84a5a",
   "metadata": {},
   "source": [
    "Each tree in the Random Forest is trained on a different subset of the training data, created by randomly sampling (with replacement) from the dataset. This is known as the bootstrap sample.\n",
    "By training each tree on a different bootstrap sample, the Random Forest ensures that the trees are decorrelated and capture different patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83b471",
   "metadata": {},
   "source": [
    "# Random Subset of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20378eb",
   "metadata": {},
   "source": [
    "During the training process, Random Forest also randomly selects a subset of features at each split point in a tree.\n",
    "This prevents highly correlated features from dominating the model, further reducing overfitting and making the model more diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3274e8a7",
   "metadata": {},
   "source": [
    "# How Is the Final Decision Made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c34ea6",
   "metadata": {},
   "source": [
    "In classification tasks, each tree in the forest votes for a class label, and the final prediction is based on majority voting across all trees.\n",
    "In regression tasks, each tree outputs a numeric value, and the final prediction is the average of all the tree outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffc507e",
   "metadata": {},
   "source": [
    "# No Need for Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0157e1",
   "metadata": {},
   "source": [
    "Since each tree is trained on a bootstrap sample, approximately one-third of the data is left out of the training set for that tree. This is known as the out-of-bag sample.\n",
    "The model can evaluate its performance on the out-of-bag samples without needing a separate validation set, providing an estimate of the generalization error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229520b",
   "metadata": {},
   "source": [
    "# Measurement of Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080e69d",
   "metadata": {},
   "source": [
    "Random Forest provides a measure of feature importance by looking at how much a feature reduces impurity (like Gini impurity or entropy) in the trees.\n",
    "Features that consistently help to improve prediction accuracy across many trees are deemed more important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119388c5",
   "metadata": {},
   "source": [
    "# Important Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a643709a",
   "metadata": {},
   "source": [
    "n_estimators:\n",
    "\n",
    "The number of decision trees in the forest.\n",
    "More trees generally improve accuracy but increase computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ad819",
   "metadata": {},
   "source": [
    "max_depth:\n",
    "\n",
    "The maximum depth of each tree. A deeper tree might capture more patterns but risks overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60ed55",
   "metadata": {},
   "source": [
    "min_samples_split:\n",
    "\n",
    "The minimum number of samples required to split a node.\n",
    "Increasing this value can reduce overfitting by making trees less complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3903d",
   "metadata": {},
   "source": [
    "min_samples_leaf:\n",
    "\n",
    "The minimum number of samples required to be at a leaf node.\n",
    "A larger number helps to smooth predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6460a88",
   "metadata": {},
   "source": [
    "max_features:\n",
    "\n",
    "The maximum number of features to consider when looking for the best split. This can be set as a proportion of the total number of features or a fixed number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d693d0",
   "metadata": {},
   "source": [
    "# Why We Love Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f968ee",
   "metadata": {},
   "source": [
    "1. Robust to Overfitting: By averaging multiple trees, Random Forest reduces the risk of overfitting, which is a common problem with individual decision trees.\n",
    "\n",
    "2. Handles Missing Data: Random Forests can handle missing values in both training and test data without much performance loss.\n",
    "\n",
    "3. Good Performance with Imbalanced Data: Random Forest performs well on datasets with class imbalance by averaging results across all trees.\n",
    "\n",
    "4. Feature Importance: It provides an inherent way to measure the importance of each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb2905",
   "metadata": {},
   "source": [
    "# Nobody's Perfect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418c299b",
   "metadata": {},
   "source": [
    "1. Complexity: While decision trees are simple and easy to interpret, Random Forests are more difficult to interpret because they combine many trees.\n",
    "\n",
    "2. Training Time: Random Forest can be slower to train than a single decision tree, especially as the number of trees grows.\n",
    "\n",
    "3. Prediction Time: While the training phase can be parallelized, making predictions can be slower, especially for large forests, since each tree must be evaluated.\n",
    "\n",
    "4. Memory Usage: Random Forests require more memory due to the multiple trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa04724",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63052758",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faeb5e9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4685eea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5901b6fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7d9dd29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "925e6454",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5577d93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d9d842c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fed03c5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd0d32bb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_purpose_venv",
   "language": "python",
   "name": "all_purpose_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
