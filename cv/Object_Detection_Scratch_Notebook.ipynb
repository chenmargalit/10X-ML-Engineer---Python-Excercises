{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc71c79",
   "metadata": {},
   "source": [
    "# Building an Object Detection Algorithm from Scratch\n",
    "In this notebook, we will implement a simplified object detection algorithm from scratch using PyTorch. We will create a model, define a dataset, and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2074a0",
   "metadata": {},
   "source": [
    "## Step 1: Set Up the Environment\n",
    "We start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aef6e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f3ab67",
   "metadata": {},
   "source": [
    "## Step 2: Define the Model\n",
    "We will define a simple object detection model with a backbone CNN for feature extraction and a prediction head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f13c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleObjectDetector(nn.Module):\n",
    "    def __init__(self, num_classes=2, grid_size=7, bbox_per_cell=2):\n",
    "        super(SimpleObjectDetector, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.grid_size = grid_size\n",
    "        self.bbox_per_cell = bbox_per_cell\n",
    "\n",
    "        # Backbone CNN\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Prediction head\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * (grid_size // 2) * (grid_size // 2), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, grid_size * grid_size * (bbox_per_cell * 5 + num_classes))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x.view(-1, self.grid_size, self.grid_size, self.bbox_per_cell * 5 + self.num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26104e14",
   "metadata": {},
   "source": [
    "## Step 3: Define the Dataset\n",
    "We will create a custom dataset class that returns images and corresponding bounding boxes and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7428f518",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, annotations, transform=None):\n",
    "        self.images = images\n",
    "        self.annotations = annotations\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        bbox = self.annotations[idx]['boxes']\n",
    "        labels = self.annotations[idx]['labels']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, bbox, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf31be8a",
   "metadata": {},
   "source": [
    "## Step 4: Define the Loss Function\n",
    "We need to calculate losses for both the bounding boxes and the class predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3fde1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, targets, grid_size, num_classes, bbox_per_cell):\n",
    "    # Placeholder for loss computation\n",
    "    return torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3097ac",
   "metadata": {},
   "source": [
    "## Step 5: Train the Model\n",
    "We will initialize the dataset, model, optimizer, and dataloader, and define a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f698101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [torch.rand(3, 224, 224) for _ in range(10)]  # Random images\n",
    "annotations = [{'boxes': torch.tensor([[50, 50, 100, 100]]), 'labels': torch.tensor([1])} for _ in range(10)]\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(images, annotations)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Model, optimizer, and loss\n",
    "model = SimpleObjectDetector(num_classes=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "417a3167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "for img, bbox, labels in dataloader:\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea34e45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a7335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "758ac4cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m      5\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(predictions, bboxes, grid_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, bbox_per_cell\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for batch_idx, (images, bboxes, labels) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        loss = compute_loss(predictions, bboxes, grid_size=7, num_classes=2, bbox_per_cell=2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Batch {batch_idx}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a228ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class SimpleObjectDetector(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleObjectDetector, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.flatten_size = 64 * 56 * 56  # Adjust based on image size and conv layers\n",
    "        self.fc = nn.Linear(self.flatten_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)  # Apply convolutions\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)  # Apply fully connected layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211ad80",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Predictions\n",
    "Finally, we visualize the predictions made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02008eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_predictions(image, predictions):\n",
    "    image = image.permute(1, 2, 0).numpy()\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "image = torch.rand(3, 224, 224)  # Random image\n",
    "predictions = model(image.unsqueeze(0))\n",
    "visualize_predictions(image, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56270b",
   "metadata": {},
   "source": [
    "# YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949b958",
   "metadata": {},
   "source": [
    "YOLO <a href='https://arxiv.org/pdf/1506.02640'>paper</a> by Facebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6095bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_img = 'dirtbike.jpg'\n",
    "piano_img = 'piano.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b470bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')  # You can use other models like 'yolov8s.pt', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a281fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(bike_img)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f959d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform object detection\n",
    "%time results = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0315141d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first result (since results is a list)\n",
    "result = results[0]\n",
    "\n",
    "# Visualize predictions (bounding boxes, labels)\n",
    "annotated_image = result.plot()  # `plot()` returns an annotated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1bc3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the annotated image\n",
    "plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8c084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optional: Print detection details\n",
    "for box, cls, conf in zip(result.boxes.xyxy, result.boxes.cls, result.boxes.conf):\n",
    "    print(f\"Class: {model.names[int(cls)]}, Confidence: {conf:.2f}, Box: {box.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95de7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxed_img(img_path: str):\n",
    "    image = cv2.imread(img_path)\n",
    "#     image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = model(image)[0]\n",
    "    annotated_image = results.plot()  \n",
    "    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba247183",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxed_img(piano_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb37776",
   "metadata": {},
   "source": [
    "## How does Yolo work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a00181b",
   "metadata": {},
   "source": [
    "YOLO divides the image into grids (squares). For each grid it predicts the probability of any given class, for example 20 classes. If the probability is not above some threshold, nothing is predicted as true\n",
    "\n",
    "Once it DID predict something as true, if the object is not in the grid's center, it won't predict anything, although there IS something there. Once it DOES find an object that its center is in the grid, it predicts the following things:\n",
    "\n",
    "1. The center offset, relative to the cell\n",
    "2. The bounding box size (width and height) of the object\n",
    "3. The class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069e9a6a",
   "metadata": {},
   "source": [
    "YOLO implementation written by ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with two convolutional layers.\"\"\"\n",
    "    def __init__(self, in_channels, filters):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, filters, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.conv2 = nn.Conv2d(filters, in_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        return self.relu(x + residual)\n",
    "\n",
    "class DarknetBlock(nn.Module):\n",
    "    \"\"\"Darknet block with N residual blocks.\"\"\"\n",
    "    def __init__(self, in_channels, filters, num_blocks):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, filters, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(filters),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            *[ResidualBlock(filters, filters // 2) for _ in range(num_blocks)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class Darknet53(nn.Module):\n",
    "    \"\"\"Backbone: Darknet-53.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "        )\n",
    "        self.block1 = DarknetBlock(32, 64, 1)\n",
    "        self.block2 = DarknetBlock(64, 128, 2)\n",
    "        self.block3 = DarknetBlock(128, 256, 8)\n",
    "        self.block4 = DarknetBlock(256, 512, 8)\n",
    "        self.block5 = DarknetBlock(512, 1024, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        x = self.initial_conv(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        outputs.append(x)  # First scale\n",
    "        x = self.block3(x)\n",
    "        outputs.append(x)  # Second scale\n",
    "        x = self.block4(x)\n",
    "        outputs.append(x)  # Third scale\n",
    "        x = self.block5(x)\n",
    "        return outputs\n",
    "\n",
    "class YOLOHead(nn.Module):\n",
    "    \"\"\"YOLO Head for bounding box and class prediction.\"\"\"\n",
    "    def __init__(self, in_channels, num_anchors, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 2, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels // 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels // 2, num_anchors * (5 + num_classes), kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.LeakyReLU(0.1)(self.bn1(self.conv1(x)))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    \"\"\"YOLOv3 Model.\"\"\"\n",
    "    def __init__(self, num_classes=80, num_anchors=3):\n",
    "        super().__init__()\n",
    "        self.backbone = Darknet53()\n",
    "        self.head1 = YOLOHead(1024, num_anchors, num_classes)\n",
    "        self.head2 = YOLOHead(512, num_anchors, num_classes)\n",
    "        self.head3 = YOLOHead(256, num_anchors, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        backbone_outputs = self.backbone(x)\n",
    "        head1_out = self.head1(backbone_outputs[-1])  # Largest scale\n",
    "        head2_out = self.head2(backbone_outputs[-2])  # Medium scale\n",
    "        head3_out = self.head3(backbone_outputs[-3])  # Smallest scale\n",
    "        return head1_out, head2_out, head3_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f86dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "model = YOLOv3(num_classes=80, num_anchors=3)\n",
    "dummy_input = torch.randn(1, 3, 416, 416)\n",
    "output = model(dummy_input)\n",
    "for out in output:\n",
    "    print(out.shape)  # Output shapes for 3 scales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a60f6b",
   "metadata": {},
   "source": [
    "YOLO uses a skip connection layer, which basically means moving a layer's output straight to another layer input. For example if I have 5 layers, I'll take the output of layer 3 and put it straight into layer 5 without passing through layer 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6b60a",
   "metadata": {},
   "source": [
    "A special kind of skip connection is the residual connection layer, which basically means adding the non processed input to the processed input, and together they make an output.\n",
    "\n",
    "For example I have 5 layers. Layer 2 gets input, does something and outputs it. In a skip connection layer, the 3rd layer will get both the processed input and (summed) the raw input to layer 2.\n",
    "\n",
    "It helps especially with vanishing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9845212f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb57f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbe734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all_purpose_venv",
   "language": "python",
   "name": "all_purpose_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
