{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1003830,"sourceType":"datasetVersion","datasetId":550917},{"sourceId":200437,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":170997,"modelId":193313}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image, UnidentifiedImageError\nfrom torchvision import datasets, models, transforms","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:41.250372Z","iopub.execute_input":"2024-12-26T02:01:41.250932Z","iopub.status.idle":"2024-12-26T02:01:43.508538Z","shell.execute_reply.started":"2024-12-26T02:01:41.250873Z","shell.execute_reply":"2024-12-26T02:01:43.507813Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"root_dir = \"../input/microsoft-catsvsdogs-dataset/PetImages\"","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:43.509851Z","iopub.execute_input":"2024-12-26T02:01:43.510245Z","iopub.status.idle":"2024-12-26T02:01:43.514427Z","shell.execute_reply.started":"2024-12-26T02:01:43.510216Z","shell.execute_reply":"2024-12-26T02:01:43.513501Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class CatsAndDogsDataset(Dataset):\n    def __init__(self, root_dir, transform=None, start=0, finish=1000):\n        self.root_dir = root_dir\n        self.transform = transform\n        \n        self.dog_files = os.listdir(os.path.join(root_dir, \"Dog\"))[start:finish]\n        self.cat_files = os.listdir(os.path.join(root_dir, \"Cat\"))[start:finish]\n        \n        self.length = min(len(self.dog_files), len(self.cat_files))\n    \n    def __len__(self):\n        return self.length * 2\n    \n\n    def __getitem__(self, idx):\n        try:\n            if idx % 2 == 0:\n                folder = \"Dog\"\n                image_files = self.dog_files\n                label = 1\n            else:\n                folder = \"Cat\"\n                image_files = self.cat_files\n                label = 0  # Cat label\n            \n            adjusted_idx = idx // 2\n            img_path = os.path.join(self.root_dir, folder, image_files[adjusted_idx])\n            \n            image = Image.open(img_path).convert(\"RGB\")\n            \n            if self.transform:\n                image = self.transform(image)\n            \n            return image, label\n            \n        except (UnidentifiedImageError, OSError) as e:\n            print(f\"Skipping corrupted image: {img_path}\")\n            return self.__getitem__((idx + 2) % len(self))\n","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:43.515571Z","iopub.execute_input":"2024-12-26T02:01:43.515884Z","iopub.status.idle":"2024-12-26T02:01:43.527066Z","shell.execute_reply.started":"2024-12-26T02:01:43.515831Z","shell.execute_reply":"2024-12-26T02:01:43.526074Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"transform = {\n    'train': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:43.528713Z","iopub.execute_input":"2024-12-26T02:01:43.528997Z","iopub.status.idle":"2024-12-26T02:01:43.540995Z","shell.execute_reply.started":"2024-12-26T02:01:43.528960Z","shell.execute_reply":"2024-12-26T02:01:43.540334Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"bs = 64\n\ntrain_dataset = CatsAndDogsDataset(root_dir=root_dir, transform=transform['train'],finish=8000)\nval_dataset = CatsAndDogsDataset(root_dir=root_dir, transform=transform['val'], start=8000, finish=9000)\n\ntrain_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T02:01:43.541988Z","iopub.execute_input":"2024-12-26T02:01:43.542322Z","iopub.status.idle":"2024-12-26T02:01:43.566238Z","shell.execute_reply.started":"2024-12-26T02:01:43.542296Z","shell.execute_reply":"2024-12-26T02:01:43.565605Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = models.resnet18(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:43.567145Z","iopub.execute_input":"2024-12-26T02:01:43.567387Z","iopub.status.idle":"2024-12-26T02:01:43.820190Z","shell.execute_reply.started":"2024-12-26T02:01:43.567363Z","shell.execute_reply":"2024-12-26T02:01:43.819371Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:44.189949Z","iopub.execute_input":"2024-12-26T02:01:44.190319Z","iopub.status.idle":"2024-12-26T02:01:44.198076Z","shell.execute_reply.started":"2024-12-26T02:01:44.190287Z","shell.execute_reply":"2024-12-26T02:01:44.197147Z"},"trusted":true},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"num_features = model.fc.in_features\nnum_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T02:01:44.353503Z","iopub.execute_input":"2024-12-26T02:01:44.353873Z","iopub.status.idle":"2024-12-26T02:01:44.359470Z","shell.execute_reply.started":"2024-12-26T02:01:44.353842Z","shell.execute_reply":"2024-12-26T02:01:44.358626Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"512"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"model.fc = nn.Linear(num_features, 2)  # 2 classes: dog, cat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T02:01:44.534024Z","iopub.execute_input":"2024-12-26T02:01:44.534444Z","iopub.status.idle":"2024-12-26T02:01:44.539432Z","shell.execute_reply.started":"2024-12-26T02:01:44.534410Z","shell.execute_reply":"2024-12-26T02:01:44.538467Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:44.743789Z","iopub.execute_input":"2024-12-26T02:01:44.744160Z","iopub.status.idle":"2024-12-26T02:01:44.903745Z","shell.execute_reply.started":"2024-12-26T02:01:44.744128Z","shell.execute_reply":"2024-12-26T02:01:44.902853Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# # Freeze all layers except the final fully connected layer\n# for param in model.parameters():\n#     param.requires_grad = False  # Freeze all parameters\n\n# # Unfreeze the final layer\n# model.fc = nn.Linear(model.fc.in_features, 2)\n# model.fc.requires_grad = True\n# model = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T02:01:47.024802Z","iopub.execute_input":"2024-12-26T02:01:47.025173Z","iopub.status.idle":"2024-12-26T02:01:47.028952Z","shell.execute_reply.started":"2024-12-26T02:01:47.025143Z","shell.execute_reply":"2024-12-26T02:01:47.028055Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # Freeze all layers initially\n# for param in model.parameters():\n#     param.requires_grad = False\n\n# # Unfreeze the last few layers\n# for name, param in model.named_parameters():\n#     if \"layer4\" in name or \"fc\" in name:  # Unfreeze layer4 (last conv block) and fc\n#         param.requires_grad = True\n\n# # Update model to the device\n# model = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T02:01:47.202915Z","iopub.execute_input":"2024-12-26T02:01:47.203784Z","iopub.status.idle":"2024-12-26T02:01:47.207567Z","shell.execute_reply.started":"2024-12-26T02:01:47.203743Z","shell.execute_reply":"2024-12-26T02:01:47.206663Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"%%time\n\nepochs = 1\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, preds = torch.max(outputs, 1)\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\n    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader)}, Accuracy: {correct/total:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:01:47.566378Z","iopub.execute_input":"2024-12-26T02:01:47.567236Z","iopub.status.idle":"2024-12-26T02:03:29.284717Z","shell.execute_reply.started":"2024-12-26T02:01:47.567198Z","shell.execute_reply":"2024-12-26T02:03:29.283751Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n  warnings.warn(str(msg))\n","output_type":"stream"},{"name":"stdout","text":"Skipping corrupted image: ../input/microsoft-catsvsdogs-dataset/PetImages/Dog/11702.jpg\nEpoch 1/1, Loss: 0.14303743577003478, Accuracy: 0.94\nCPU times: user 2min 53s, sys: 15.2 s, total: 3min 8s\nWall time: 1min 41s\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"model.eval()\nval_correct = 0\nval_total = 0\n\nwith torch.no_grad():\n    for inputs, labels in val_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        val_correct += (preds == labels).sum().item()\n        val_total += labels.size(0)\n\nprint(f\"Validation Accuracy: {val_correct/val_total:.2f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-12-26T02:03:29.286271Z","iopub.execute_input":"2024-12-26T02:03:29.286554Z","iopub.status.idle":"2024-12-26T02:03:40.204546Z","shell.execute_reply.started":"2024-12-26T02:03:29.286526Z","shell.execute_reply":"2024-12-26T02:03:40.203622Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Skipping corrupted image: ../input/microsoft-catsvsdogs-dataset/PetImages/Dog/Thumbs.db\nSkipping corrupted image: ../input/microsoft-catsvsdogs-dataset/PetImages/Cat/Thumbs.db\nValidation Accuracy: 0.93\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"dog = f'{root_dir}/Dog/3863.jpg'\ncat = f'{root_dir}/Cat/5307.jpg'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def return_pred(img_path: str):\n    preprocessed_image = transform['val'](img).unsqueeze(0)\n    return model(preprocessed_image.to(device)).argmax(1)    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Image.open(dog)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"return_pred(dog)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Image.open(cat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"return_pred(cat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When should I only train the last linear layer?\n1. Small dataset - to avoid overfitting\n2. Similar task - it might not make sense\n3. When I don't want to spend much computations","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}