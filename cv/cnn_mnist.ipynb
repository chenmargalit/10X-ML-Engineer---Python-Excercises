{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a181bb1",
   "metadata": {},
   "source": [
    "# Image Classification in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de7202",
   "metadata": {},
   "source": [
    "\n",
    "## What is Image Classification?\n",
    "Image classification is a process where a computer learns to assign labels to images based on their content. It is a key task in computer vision with applications ranging from medical imaging to autonomous vehicles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df52ef",
   "metadata": {},
   "source": [
    "\n",
    "## Steps in Image Classification\n",
    "1. **Data Collection**: Gather labeled images.\n",
    "2. **Data Preprocessing**: Resize, normalize, and augment images.\n",
    "3. **Model Selection**: Choose a neural network architecture (e.g., CNN).\n",
    "4. **Training**: Train the model using a dataset.\n",
    "5. **Evaluation**: Test the model on unseen data.\n",
    "6. **Inference**: Use the trained model to classify new images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db7034",
   "metadata": {},
   "source": [
    "\n",
    "## Key Concepts\n",
    "- **Convolutional Neural Networks (CNNs)**: The most common architecture for image classification.\n",
    "- **Overfitting**: When the model performs well on training data but poorly on test data.\n",
    "- **Regularization Techniques**: Dropout, data augmentation, and weight decay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7b9c5467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "276c5d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67d38f",
   "metadata": {},
   "source": [
    "Why is the image that started as 32 and halved twice, becomes 12 and not 8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5fda8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)      \n",
    "        x = x.view(-1, 64 * 12 * 12)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7b0e76fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "89142d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(CNN, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "#         self.fc1 = nn.Linear(64 * 12 * 12, 128)\n",
    "#         self.fc2 = nn.Linear(128, 10)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = self.relu(self.conv1(x))\n",
    "#         x = self.pool(self.relu(self.conv2(x)))\n",
    "#         x = x.view(-1, 64 * 12 * 12)\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# model = CNN().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "14d35c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf1bbd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1/5, Loss: 0.1304\n",
      "1\n",
      "Epoch 2/5, Loss: 0.0383\n",
      "2\n",
      "Epoch 3/5, Loss: 0.0240\n",
      "3\n",
      "Epoch 4/5, Loss: 0.0163\n",
      "4\n",
      "Epoch 5/5, Loss: 0.0121\n",
      "CPU times: user 7min, sys: 16min 5s, total: 23min 6s\n",
      "Wall time: 6min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Train the model\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(epoch)\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b4d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384e74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b21ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62143653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d04031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef99d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "959511af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[0;32m----> 7\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m      9\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97c35edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAACvCAYAAADJy0JWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcU0lEQVR4nO3deXBUVdrH8achgZBgMRASQMAkgDBiRLYgQsBBGXDCvkkqjMNSBcimDEpUlhExDFacYhlZLXVwIEYEJKJCQKjINriEAR0QHMhAICUMiYRAWIaQ3PcPi7zenAvd6XT37dP5fqr44/xy7s3TcLidJ7f7tMMwDEMAAAAAANBUDbsLAAAAAACgKmhsAQAAAABao7EFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsPSA6OlrGjBljdxmohlh7sBPrD3Zh7cFOrD/YhbV3d9o3tmvWrBGHw1H+JyQkRFq3bi1Tp06V//73v3aX59S8efNM9Vf8s3//frtLxB3ovvaOHz8uycnJ0r59e7nnnnukSZMm0q9fP8nOzra7NLhA9/UnIrJgwQIZOHCgNGrUSBwOh8ybN8/ukuCCQFh7ZWVlkpqaKjExMRISEiLt2rWT9PR0u8uCCwJh/f1SWlqaOBwOqVu3rt2lwIlAWHuB/rwbZHcBnjJ//nyJiYmRGzduyL59+2TlypWydetWOXLkiISGhtpd3h0NHTpUWrVqpeSzZs2S4uJiiYuLs6EqVIaua+/tt9+Wd955R4YNGyaTJ0+WoqIiWb16tXTt2lUyMzOld+/edpcIF+i6/kRE5syZI40bN5YOHTrI9u3b7S4HlaTz2ps9e7a8/vrrMn78eImLi5OPP/5YkpKSxOFwSGJiot3lwQU6r7/biouLJTk5WcLCwuwuBZWg89oL+OddQ3N/+9vfDBExvvnmG1M+Y8YMQ0SM999//47HFhcXe6SGqKgoY/To0R45l2EYxpkzZwyHw2GMHz/eY+eE5+m+9rKzs40rV66YsoKCAiMiIsLo3r27B6qDN+m+/gzDME6dOmUYhmHk5+cbImK88sorHqkL3qX72svLyzOCg4ONKVOmlGdlZWVGjx49jGbNmhm3bt3ySI3wDt3X3y+9+OKLRps2bYxRo0YZYWFhVS8MXhUIay/Qn3e1fynynTz++OMiInLq1CkRERkzZozUrVtXcnJyJCEhQe655x4ZNWqUiPz8kqQlS5bIgw8+KCEhIdKoUSOZOHGiFBYWms5pGIakpKRIs2bNJDQ0VHr16iVHjx61/P45OTmSk5PjVu3p6eliGEZ5fdCLLmuvU6dOykufwsPDpUePHnLs2LFKP274B13Wn8jP7xVC4NBl7X388cdSUlIikydPLs8cDodMmjRJ8vLy5MCBA249fthLl/V324kTJ2Tx4sWyaNEiCQoKmBdQVks6rb1Af94N2P9Jt/+Bw8PDy7Nbt25J3759JT4+Xv7yl7+Uv1xg4sSJsmbNGhk7dqw8++yzcurUKVm2bJkcOnRI9u/fL8HBwSIi8qc//UlSUlIkISFBEhIS5J///Kf06dNHbt68qXz/J554QkRETp8+Xena09LSpHnz5tKzZ89KHwv76bz2RETOnz8vDRs2dOtY2E/39Qd96bL2Dh06JGFhYfLAAw+Y8i5dupR/PT4+3r2/BNhGl/V32/Tp06VXr16SkJAgH374YVUeOmym29oLaDbeLfaI2y8L2Llzp5Gfn2+cPXvW+OCDD4zw8HCjTp06Rl5enmEYhjF69GhDRIyXXnrJdPzevXsNETHS0tJMeWZmpim/cOGCUatWLaNfv35GWVlZ+bxZs2YZIqK8LCAqKsqIioqq9OM5cuSIISJGcnJypY+FbwXa2jMMw9izZ4/hcDiMuXPnunU8fCeQ1l+gviQqUOm+9vr162e0aNFCya9evWpZL/yL7uvPMAzj008/NYKCgoyjR4+W18pLkf1fIKy92wL1eTdgXorcu3dviYiIkObNm0tiYqLUrVtXNm/eLE2bNjXNmzRpkmm8YcMGqVevnvz2t7+VgoKC8j+3X6aZlZUlIiI7d+6UmzdvyrRp08ThcJQfP336dMt6Tp8+7fbdWhHhZcgaCZS1d+HCBUlKSpKYmBhJTk6u9PGwR6CsP+hH17V3/fp1qV27tpKHhISUfx3+T9f1d/PmTfnjH/8ozzzzjLRt27ZyDxp+Qde1Vx0EzEuRly9fLq1bt5agoCBp1KiRtGnTRmrUMPftQUFB0qxZM1N24sQJKSoqksjISMvzXrhwQUREcnNzRUTk/vvvN309IiJC6tev75HHYBiGvP/++xIbGyvt2rXzyDnhfYGw9q5evSr9+/eXK1euyL59+/jYAY0EwvqDnnRde3Xq1JH//e9/Sn7jxo3yr8P/6br+Fi9eLAUFBfLqq6+6fQ7YS9e1Vx0ETGPbpUsX6dy5813n1K5dW1l4ZWVlEhkZWX6ntKKIiAiP1ejM/v37JTc3VxYuXOiz74mq033t3bx5U4YOHSrfffedbN++XWJjY33yfeEZuq8/6EvXtdekSRPJysoSwzBMd0POnTsnIiL33nuvV78/PEPH9VdUVCQpKSkyefJkuXz5sly+fFlEfv7YH8Mw5PTp0xIaGnrHxgf+Qce1V10ETGPrrpYtW8rOnTule/fud/0tbVRUlIj8/NuWFi1alOf5+fnKTmbuuv0h3UlJSR45H/ybP6y9srIy+cMf/iC7du2SDz/8UB577LEqnQ/68If1h+rJ7rXXvn17efvtt+XYsWOml4J+9dVX5V9H4LJz/RUWFkpxcbGkpqZKamqq8vWYmBgZNGiQZGRkuHV++De7r33VQcC8x9ZdTz31lJSWlsprr72mfO3WrVty6dIlEfn59fTBwcHy5ptvimEY5XOWLFlied7KbvteUlIiGzZskPj4eLnvvvsq9RigJ39Ye9OmTZP169fLihUrZOjQoZV+DNCXP6w/VE92r71BgwZJcHCwrFixojwzDENWrVolTZs2lW7dulXuAUErdq6/yMhI2bx5s/KnV69eEhISIps3b5aXX37Z7ccG/2b3ta86qPZ3bB977DGZOHGiLFy4UA4fPix9+vSR4OBgOXHihGzYsEGWLl0qw4cPl4iICHnhhRdk4cKF0r9/f0lISJBDhw7Jtm3bLD8apbJbb2/fvl1++uknNo2qRuxee0uWLJEVK1bIo48+KqGhobJu3TrT14cMGSJhYWEee7zwL3avPxGRtWvXSm5urly7dk1ERPbs2SMpKSkiIvL000+X/9YagcXutdesWTOZPn26vPHGG1JSUiJxcXGSkZEhe/fulbS0NKlZs6Y3Hjb8hJ3rLzQ0VAYPHqzkGRkZ8vXXX1t+DYHD7mufSOA/71b7xlZEZNWqVdKpUydZvXq1zJo1S4KCgiQ6Olp+//vfS/fu3cvnpaSkSEhIiKxatUqysrLkkUcekR07dki/fv2qXENaWpoEBwfLiBEjqnwu6MPOtXf48GERETlw4IAcOHBA+fqpU6dobAOc3de+d955R3bv3l0+zsrKKt8VMj4+XvsnWNyZ3Wvv9ddfl/r168vq1atlzZo1cv/998u6det4K1A1Yff6Q/Vl99oL9Oddh/HLe9wAAAAAAGim2r/HFgAAAACgNxpbAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWaGwBAAAAAFoLcnWiw+HwZh3QkK8+Apm1h4p8+fHbrD9UxLUPduHaBztx7YNdXF173LEFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABao7EFAAAAAGgtyO4CgED2wgsvKFmdOnWUrF27dko2fPhwp+dfuXKlkh04cEDJ1q5d6/RcAAAAgK64YwsAAAAA0BqNLQAAAABAazS2AAAAAACt0dgCAAAAALTmMAzDcGmiw+HtWqAZF5dOlem09tavX28au7IBlKfl5OQoWe/evU3jM2fO+Kocr/DV2hPRa/35g9atWyvZ8ePHTePnnntOmfPmm296rSZP49rnOWFhYUr2xhtvmMYTJ05U5hw8eFDJRowYoWS5ublVqM7/cO2Dnbj2wS6urj3u2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK0F2V0AoKuKG0WJuL9ZVMXNdUREtm/fbhq3aNFCmTNgwAAla9mypZKNGjXKNF64cGFlSwRc0qFDByUrKyszjfPy8nxVDvxckyZNlGz8+PGmccX1IyLSqVMnJevfv7+SLV++vArVQVcdO3ZUso8++kjJoqOjfVDN3fXp00fJjh07pmRnz571RTnQkNXPglu2bDGNp06dqsxZtWqVkpWWlnquMBtwxxYAAAAAoDUaWwAAAACA1mhsAQAAAABa4z22gAs6d+6sZEOGDHF63NGjR5Vs4MCBSlZQUKBkxcXFpnGtWrWUOV9++aWSPfzww0oWHh5+1zoBT2nfvr2SXb161TTevHmzj6qBP4mIiFCy9957z4ZKEOj69u2rZLVr17ahEues3h85btw4JUtMTPRFOfBzVj/PrVixwulxy5YtU7J3331Xya5fv+5eYX6CO7YAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrfrt51PDhw5Ws4oe2i4j8+OOPSnbjxg3TOC0tTZlz/vx5JTt58mRlSkQ10qRJEyVzOBxKVnGzKKsNLM6dO+dWDc8//7yStW3b1qVjP/vsM7e+J3A3sbGxSmb1IfBr1671RTnwI88++6ySDR48WMm6dOnise/Zs2dPJatRw/z7+2+//VaZs2fPHo/VAHsEBZl/nE1ISLCpkso7ePCgks2YMUPJwsLCTOOKm/KherC6zjVr1szpcenp6UpWsV8KBNyxBQAAAABojcYWAAAAAKA1GlsAAAAAgNZobAEAAAAAWvPbzaNSU1OVLDo62q1zTZw4UcmuXLmiZBU3/vEXeXl5Slbx7yc7O9tX5VRLn3zyiZK1atVKySquq4sXL3qshsTERCULDg722PmByvr1r3+tZBU3OBERWb9+vS/KgR9ZvHixkpWVlXn1ew4dOtRplpubq8wZOXKkkllt6AP/1atXL9P40UcfVeZY/VzpD+rXr69kVhtDhoaGmsZsHhX4ateurWSzZ89261xWmzgahuHWufwZd2wBAAAAAFqjsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDW/HbzqPHjxytZu3btlOzYsWNK9sADD5jGHTt2VOb85je/UbKuXbsq2dmzZ03j5s2bK3NcdevWLdM4Pz9fmdOkSROXznXmzBnTmM2jfM9qExJPmjlzpmncunVrl4776quvXMqAqkpOTlYyq/8XXJ8C39atW03jGjW8+3vzn376ScmKi4uVLCoqyjSOiYlR5nz99ddKVrNmzSpUB2+KjY1VsvT0dNM4JydHmfPnP//ZazVVxaBBg+wuAX7qoYceUrJOnTq5dGzFnmPbtm0eqcnfcccWAAAAAKA1GlsAAAAAgNZobAEAAAAAWvPb99ju2rXLpcxKZmam0zlWH4jdvn17Jav4Ie1xcXEu1WDlxo0bpvG///1vZY7Ve4YbNGigZFbvH4G++vfvr2Tz5883jWvVqqXMuXDhgpK9/PLLSnbt2rUqVAeIREdHK1nnzp2VzOq6dvXqVW+UBJs89thjStamTRvTuKysTJljlbli1apVSrZjxw4lKyoqUrLHH3/cNJ49e7ZL33PSpElKtnLlSpeOhXfNmTNHycLCwkzjJ598Uplj9R5sX7P6ec7q/5O7/1cQWIYNG+b2sVbXyOqAO7YAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrfrt5lLcVFhYqWVZWltPjXN3AyhVWbwq32tTqX//6l5KtX7/eY3XAflab8FhtFlWR1TrYvXu3R2oCfslqgxMr+fn5Xq4EvmS1adgHH3ygZA0bNnTr/Lm5uUq2adMm0/jVV19V5ri6IV7F80+YMEGZExERoWSpqalKFhISYhovW7ZMmVNSUuJSXXDN8OHDlSwhIUHJTp48aRpnZ2d7raaqsNq8zGqjqC+++ELJLl265IWK4M969uzp0rybN28qmasb5QUa7tgCAAAAALRGYwsAAAAA0BqNLQAAAABAazS2AAAAAACtVdvNo+wQGRlpGq9YsUKZU6OG+ruG+fPnK9nFixc9Vxh8KiMjQ8n69Onj9Li///3vSjZnzhxPlAQ49dBDD7k0z2rTHegrKEj9McHdjaKsNrZLTExUsoKCArfOb6Xi5lELFy5U5ixatEjJQkNDlazi2t6yZYsyJycnp7Il4i5GjBihZFb/NlY/T/mDipuvjRo1SplTWlqqZCkpKUrGxmSBr1u3bncd38nVq1eV7PDhw54oSTvcsQUAAAAAaI3GFgAAAACgNRpbAAAAAIDWaGwBAAAAAFpj8ygfmjJlimkcERGhzCksLFSyH374wWs1wbuaNGmiZFabAdSuXVvJKm6gYrWZRHFxcRWqA+6sa9eupvHYsWOVOYcOHVKyzz//3Gs1QR/Z2dlKNm7cOCXz5EZRrrDa8MlqQ5+4uDhflINfqFevnpJVvA7dycqVKz1djkdMmDDBNLbaeO3YsWNKlpWV5bWa4L/cve746/q3A3dsAQAAAABao7EFAAAAAGiNxhYAAAAAoDXeY+sl3bt3V7KXXnrJ6XGDBw9WsiNHjniiJNhg06ZNShYeHu7SsevWrTONc3JyPFIT4IrevXubxg0aNFDmZGZmKtmNGze8VhP8Q40azn8n/sgjj/igkspzOBxKZvV4XHmM8+bNU7Knn37arbpgvddE06ZNlSw9Pd0X5XhEy5Ytnc7hZzzc1rlzZ6dzLl26pGS8x/b/cccWAAAAAKA1GlsAAAAAgNZobAEAAAAAWqOxBQAAAABojc2jvCQhIUHJgoODTeNdu3Ypcw4cOOC1muB9AwcONI07duzo0nFffPGFkr3yyiueKAlwy8MPP2waG4ahzNm4caOvyoFNnnnmGSUrKyuzoRLPGDBggJJ16NBByaweY8XMavMouO/KlStKdvjwYSVr166dklXc3O7ixYseq8tVkZGRSjZ8+HCnx+3bt88b5cDPxcfHK1lSUpLT44qKipQsLy/PIzUFAu7YAgAAAAC0RmMLAAAAANAajS0AAAAAQGs0tgAAAAAArbF5lAfUqVNHyZ588kklu3nzpmlstTlQSUmJ5wqDV4WHhyvZrFmzTOOKG4bdidUGGcXFxW7VBVRW48aNlaxHjx6m8Q8//KDM2bx5s9dqgn+w2mzJX0VERChZ27ZtTeOK1+jKyM/PN415vvas69evK1lOTo6SDRs2TMk+++wz03jRokUeqys2NlbJWrRooWTR0dFKZrXpXkU6b8YG91n9DFmjhvP7jZ9//rk3ygkY3LEFAAAAAGiNxhYAAAAAoDUaWwAAAACA1mhsAQAAAABaY/MoD5g5c6aSdejQQckyMzNN43/84x9eqwne9/zzzytZXFyc0+MyMjKUzGojMcBXxowZo2SRkZGm8bZt23xUDeCe2bNnK9mUKVPcOtfp06eVbPTo0abxmTNn3Do3XGf13OhwOJSsX79+pnF6errHaigoKFAyq02hGjZs6Nb516xZ49Zx0Nvw4cOdzrl06ZKSrV692gvVBA7u2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrvMe2kiq+j0NEZO7cuUp2+fJlJZs/f75XaoI9ZsyY4dZxU6dOVbLi4uKqlgO4LSoqyumcwsJCH1QCuGbr1q1K1qZNG4+d//vvv1eyffv2eez8cM3x48eV7KmnnlKy9u3bm8atWrXyWA0bN250ad57772nZKNGjXJ63PXr1ytdE/TSrFkzJUtKSnJ6XF5enpJlZ2d7pKZAxR1bAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgNTaPciI8PNw0/utf/6rMqVmzppJZbWzx5Zdfeq4waKtBgwZKVlJS4rHzFxUVOT1/cHCwMqdevXounf9Xv/qVaezuJloiIqWlpabxiy++qMy5du2a2+eHa/r37+90zieffOKDSuBvHA6HktWo4fx34r/73e9cOv9bb72lZPfee6/T46xqKCsrc+l7umLAgAEeOxe87/Dhw3cd+8J//vMft46LjY1VsiNHjlS1HPiRbt26KZkr19GMjAwvVBPYuGMLAAAAANAajS0AAAAAQGs0tgAAAAAArdHYAgAAAAC0xuZRv2C1CVRmZqZpHBMTo8zJyclRsrlz53quMASU7777zqvn37Bhg5KdO3fONG7UqJEyZ+TIkV6ryVXnz59XsgULFthQSeCKj49XssaNG9tQCXSwcuVKJUtNTXV63Keffqpkrm7u5O4mUO4et2rVKreOA37JaqM1q6wiNooKfBU3or2TgoIC03jp0qXeKCegcccWAAAAAKA1GlsAAAAAgNZobAEAAAAAWqOxBQAAAABojc2jfqFly5ZK1qlTJ6fHzZgxQ8msNpRCYNm6dauSDRo0yIZKzEaMGOGxc926dUvJXNmgZcuWLUqWnZ3t9Li9e/e6VhjcNmTIECWz2jjv0KFDpvGePXu8VhP810cffaRkM2fOVLKIiAhflHNX+fn5Snbs2DHTeMKECcqcipvrAe4wDMOlDNVP3759XZp35swZ07ioqMgb5QQ07tgCAAAAALRGYwsAAAAA0BqNLQAAAABAa9X2PbZRUVFKtmPHDqfHWb23yOqD6BH4hg4dqmTJycmmcXBwsNvnf/DBB03jkSNHun2ud9991zQ+ffq0S8dt2rRJyY4fP+52HfCt0NBQJUtISHDp2I0bN5rGpaWlHqkJesnNzVWyxMREJRs8eLBp/Nxzz3mrpDtasGCBki1fvtzndaB6CgkJcTrn+vXrPqgEdrL6uc9qDx8rN27cMI1LSko8UlN1wh1bAAAAAIDWaGwBAAAAAFqjsQUAAAAAaI3GFgAAAACgtWq7eZTVh7Tfd999To/bvXu3kvEB3LgtNTXVa+dOSkry2rkRmKw2nigsLFSyLVu2KNnSpUu9UhP0t2fPHqeZ1WaMVs+7AwYMULKK6/Gtt95S5jgcDiX7/vvv1WIBHxk7dqySXbp0yTR+7bXXfFQN7FJWVqZk2dnZShYbG6tkJ0+e9EpN1Ql3bAEAAAAAWqOxBQAAAABojcYWAAAAAKA1GlsAAAAAgNaqxeZR8fHxSjZt2jQbKgEA37HaPKpbt242VILqJjMz06UMCBTffPONki1atMg0zsrK8lU5sElpaamSzZ49W8msNp49ePCgV2qqTrhjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK3R2AIAAAAAtFYtNo/q0aOHktWtW9elY3Nyckzj4uJij9QEAACAwDBgwAC7S4Cf+vHHH5Vs3LhxNlQS+LhjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK1Vi/fYuurbb79VsieeeMI0vnjxoq/KAQAAAAC4gDu2AAAAAACt0dgCAAAAALRGYwsAAAAA0BqNLQAAAABAaw7DMAyXJjoc3q4FmnFx6VQZaw8V+WrtibD+oOLaB7tw7YOduPbBLq6uPe7YAgAAAAC0RmMLAAAAANAajS0AAAAAQGs0tgAAAAAArbm8eRQAAAAAAP6IO7YAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK3R2AIAAAAAtEZjCwAAAADQGo0tAAAAAEBrNLYAAAAAAK39H7SX3I8raC7TAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Visualize some predictions\n",
    "import numpy as np\n",
    "\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "outputs = model(images)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize=(12, 4))\n",
    "for i in range(6):\n",
    "    axes[i].imshow(images[i].cpu().squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f\"Pred: {preds[i].item()}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fd6d50",
   "metadata": {},
   "source": [
    "\n",
    "## Tips for Success\n",
    "1. **Use Pretrained Models**: Leverage models like ResNet and EfficientNet for better performance.\n",
    "2. **Experiment with Hyperparameters**: Try different learning rates, optimizers, and architectures.\n",
    "3. **Augment Data**: Increase your dataset size artificially by applying transformations like flipping and rotation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60074512",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "- Explore transfer learning.\n",
    "- Experiment with larger datasets.\n",
    "- Implement advanced architectures like ResNet or DenseNet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
